{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7408740,"sourceType":"datasetVersion","datasetId":4309021},{"sourceId":7705966,"sourceType":"datasetVersion","datasetId":4498963}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Importing Libraries and Loading Files(Data)**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-04T19:06:01.147202Z","iopub.execute_input":"2024-03-04T19:06:01.148102Z","iopub.status.idle":"2024-03-04T19:06:01.171652Z","shell.execute_reply.started":"2024-03-04T19:06:01.148056Z","shell.execute_reply":"2024-03-04T19:06:01.170668Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"/kaggle/input/pseudo-labels/pseudo_labels-face.npy\n/kaggle/input/pseudo-labels/keys-speech.txt\n/kaggle/input/pseudo-labels/keys-ocr.txt\n/kaggle/input/pseudo-labels/pseudo_labels-audio.npy\n/kaggle/input/pseudo-labels/pseudo_labels-rgb.npy\n/kaggle/input/pseudo-labels/keys-rgb.txt\n/kaggle/input/pseudo-labels/pseudo_labels-ocr.npy\n/kaggle/input/pseudo-labels/pseudo_labels-object.npy\n/kaggle/input/pseudo-labels/keys-face.txt\n/kaggle/input/pseudo-labels/pseudo_labels-speech.npy\n/kaggle/input/pseudo-labels/pseudo_labels-scene.npy\n/kaggle/input/pseudo-labels/keys-audio.txt\n/kaggle/input/pseudo-labels/keys-object.txt\n/kaggle/input/pseudo-labels/keys-scene.txt\n/kaggle/input/pseudo-labels-rgb/pseudo_labels.npy\n/kaggle/input/pseudo-labels-rgb/keys.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_jsfusion.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_miech.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_jsfusion.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/raw-captions.pkl\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_miech.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_r2p1d_30fps_256px_stride32_offset0_inner_stride1/r2p1d-ig65m-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/audio_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/flow_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/facefeats-clone.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/resnet_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/resnext101_32x48d-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_i3d_25fps_256px_stride25_offset0_inner_stride1/i3d-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install zsvision","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:28:42.206798Z","iopub.execute_input":"2024-03-04T17:28:42.207306Z","iopub.status.idle":"2024-03-04T17:28:56.995565Z","shell.execute_reply.started":"2024-03-04T17:28:42.207271Z","shell.execute_reply":"2024-03-04T17:28:56.994292Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting zsvision\n  Obtaining dependency information for zsvision from https://files.pythonhosted.org/packages/5b/64/ed80e275858e8e88fc8c46e73d9f15c8c660cd46c1e996d50c18453e3172/zsvision-0.7.12-py3-none-any.whl.metadata\n  Downloading zsvision-0.7.12-py3-none-any.whl.metadata (897 bytes)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.24.3)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.0.5)\nRequirement already satisfied: msgpack-numpy in /opt/conda/lib/python3.10/site-packages (from zsvision) (0.4.8)\nRequirement already satisfied: typeguard in /opt/conda/lib/python3.10/site-packages (from zsvision) (2.13.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.11.4)\nCollecting mergedeep (from zsvision)\n  Obtaining dependency information for mergedeep from https://files.pythonhosted.org/packages/2c/19/04f9b178c2d8a15b076c8b5140708fa6ffc5601fb6f1e975537072df5b2a/mergedeep-1.3.4-py3-none-any.whl.metadata\n  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: humanize in /opt/conda/lib/python3.10/site-packages (from zsvision) (4.9.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from zsvision) (3.7.4)\nCollecting beartype>=0.3.2 (from zsvision)\n  Obtaining dependency information for beartype>=0.3.2 from https://files.pythonhosted.org/packages/81/4a/97ea8a5afb289a25ae7db3b3ef68f0aad892bc1756be94565154877b173e/beartype-0.17.2-py3-none-any.whl.metadata\n  Downloading beartype-0.17.2-py3-none-any.whl.metadata (30 kB)\nCollecting hickle>=4.0.0 (from zsvision)\n  Obtaining dependency information for hickle>=4.0.0 from https://files.pythonhosted.org/packages/a0/fc/f91e9b1f6f5581b09c91339832a03b4e63444eb1a3565c9d5bad4abaebdd/hickle-5.0.2-py3-none-any.whl.metadata\n  Downloading hickle-5.0.2-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from zsvision) (6.0.1)\nRequirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from hickle>=4.0.0->zsvision) (3.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->zsvision) (1.16.0)\nDownloading zsvision-0.7.12-py3-none-any.whl (25 kB)\nDownloading beartype-0.17.2-py3-none-any.whl (872 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.4/872.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading hickle-5.0.2-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\nInstalling collected packages: mergedeep, beartype, hickle, zsvision\nSuccessfully installed beartype-0.17.2 hickle-5.0.2 mergedeep-1.3.4 zsvision-0.7.12\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfrom zsvision.zs_utils import memcache, concat_features\n!pip install einops\nimport torch\nfrom torch import nn\nimport pdb\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nimport torch\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport random\nimport numpy as np\nimport os\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:28:56.996958Z","iopub.execute_input":"2024-03-04T17:28:56.997233Z","iopub.status.idle":"2024-03-04T17:28:57.323688Z","shell.execute_reply.started":"2024-03-04T17:28:56.997205Z","shell.execute_reply":"2024-03-04T17:28:57.322744Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Dataloaders**","metadata":{}},{"cell_type":"code","source":"# initializing text embedding dim\ntext_dim = 768\nclass MSRVTTTrain(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        object_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/resnext101_32x48d-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        train_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_full.txt',\n        pseudo_labels_path = \"/kaggle/input/pseudo-labels-rgb/pseudo_labels.npy\",\n        keys_path = \"/kaggle/input/pseudo-labels-rgb/keys.txt\",\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        self.object_data = memcache(object_data_path)\n        \n        #load pseudo labels\n        #rgb\n        self.rgb_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-rgb.npy\")\n        self.rgb_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-rgb.txt\", \"r\") as f:\n            for line in f:\n                self.rgb_keys.append(line.strip())\n        #audio\n        self.audio_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-audio.npy\")\n        self.audio_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-audio.txt\", \"r\") as f:\n            for line in f:\n                self.audio_keys.append(line.strip())\n        #ocr\n        self.ocr_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-ocr.npy\")\n        self.ocr_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-ocr.txt\", \"r\") as f:\n            for line in f:\n                self.ocr_keys.append(line.strip())\n        #speech\n        self.speech_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-speech.npy\")\n        self.speech_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-speech.txt\", \"r\") as f:\n            for line in f:\n                self.speech_keys.append(line.strip())\n        #scene\n        self.scene_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-scene.npy\")\n        self.scene_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-scene.txt\", \"r\") as f:\n            for line in f:\n                self.scene_keys.append(line.strip())\n        #face\n        self.face_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-face.npy\")\n        self.face_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-face.txt\", \"r\") as f:\n            for line in f:\n                self.face_keys.append(line.strip())\n        #object\n        self.object_pseudo = np.load(\"/kaggle/input/pseudo-labels/pseudo_labels-object.npy\")\n        self.object_keys = []\n        with open(\"/kaggle/input/pseudo-labels/keys-object.txt\", \"r\") as f:\n            for line in f:\n                self.object_keys.append(line.strip())\n        #load train list and find splits\n        self.split1 = []\n        self.split2 = []\n        \n        with open(train_list) as f:\n            self.train_list = f.readlines()\n        self.train_list = [x.strip() for x in self.train_list]\n        \n        for key in self.train_list:\n        # Check for modalities in the first split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False:\n                self.split1.append(key)\n            else:\n                if isinstance(self.scene_data[key], float) == False :\n                    self.split2.append(key)\n        \n\n    def __getitem__(self, index):\n        key = self.train_list[index]\n        #available_modalities = [\"Object\"]\n        \n        if key in self.split1:\n            available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\"]\n        elif key in self.split2:\n            available_modalities = [\"RGB\", \"Face\", \"Scene\", \"Object\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        #pick random label each time\n        r = random.randint(0, len(self.label[key])-1)\n        label = self.label[key][r]\n        \n        #pick the first label\n        #label = self.label[key][0]\n        \n        #------------Object------------\n        output_data[\"Object\"] = self.object_data[key]\n        if \"Object\" in available_modalities:\n            object_mask = np.ones((self.num_position, 1))\n        else:\n            object_mask = np.zeros((self.num_position, 1))\n        index = self.object_keys.index(key)\n        object_pseudo = torch.Tensor(self.object_pseudo[index])\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        index = self.audio_keys.index(key)\n        audio_pseudo = torch.Tensor(self.audio_pseudo[index])\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        index = self.rgb_keys.index(key)\n        rgb_pseudo = torch.Tensor(self.rgb_pseudo[index])\n        \n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        index = self.ocr_keys.index(key)\n        ocr_pseudo = torch.Tensor(self.ocr_pseudo[index])\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        index = self.speech_keys.index(key)\n        speech_pseudo = torch.Tensor(self.speech_pseudo[index])\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        index = self.face_keys.index(key)\n        face_pseudo = torch.Tensor(self.face_pseudo[index])\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        index = self.scene_keys.index(key)\n        scene_pseudo = torch.Tensor(self.scene_pseudo[index])\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n            \"Object\": object_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo,\n            key\n        )\n    \n    def collate_data(self, batch):\n        keys = [item[-1] for item in batch]\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        object_batch = [item[0][\"Object\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        object_masks_batch = [item[2][\"Object\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        object_pseudo_batch = [item[9] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        object_tensor = np.zeros((len(object_batch),2048))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            object_tensor[i] = object_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n            \"Object\": torch.stack([torch.from_numpy(mask).float() for mask in object_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        object_pseudo = torch.stack(object_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        output_data[\"Object\"] = torch.from_numpy(object_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo,\n            keys\n        )\n    \n    def __len__(self):\n        return len(self.train_list)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T19:08:34.168316Z","iopub.execute_input":"2024-03-04T19:08:34.168719Z","iopub.status.idle":"2024-03-04T19:08:34.225453Z","shell.execute_reply.started":"2024-03-04T19:08:34.168691Z","shell.execute_reply":"2024-03-04T19:08:34.224462Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class MSRVTTValidation(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        object_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/resnext101_32x48d-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        val_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_full.txt',\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        self.object_data = memcache(object_data_path)\n        \n        #load validation list and find splits\n        self.split = []\n        \n        with open(val_list) as f:\n            self.val_list = f.readlines()\n        self.val_list = [x.strip() for x in self.val_list]\n        \n        for key in self.val_list:\n        # Check for modalities in the split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False and isinstance(self.scene_data[key], float) == False:\n                self.split.append(key)\n\n    def __getitem__(self, index):\n        key = self.split[index]\n        \n        available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\", \"Face\", \"Scene\", \"Object\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        label = self.label[key][0]\n        \n        #------------Object------------\n        output_data[\"Object\"] = self.object_data[key]\n        if \"Object\" in available_modalities:\n            object_mask = np.ones((self.num_position, 1))\n        else:\n            object_mask = np.zeros((self.num_position, 1))\n        object_pseudo = torch.zeros((text_dim,))\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        audio_pseudo = torch.zeros((text_dim,))\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        rgb_pseudo = torch.zeros((text_dim, ))\n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        ocr_pseudo = torch.zeros((text_dim,))\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        speech_pseudo = torch.zeros((text_dim,))\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        face_pseudo = torch.zeros((text_dim,))\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        scene_pseudo = torch.zeros((text_dim,))\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n            \"Object\": object_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo\n        )\n    \n    def collate_data(self, batch):\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        object_batch = [item[0][\"Object\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        object_masks_batch = [item[2][\"Object\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        object_pseudo_batch = [item[9] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        object_tensor = np.zeros((len(object_batch), 2048))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            object_tensor[i] = object_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n            \"Object\": torch.stack([torch.from_numpy(mask).float() for mask in object_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        object_pseudo = torch.stack(object_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        output_data[\"Object\"] = torch.from_numpy(object_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo,\n        )\n    def __len__(self):\n        return len(self.split)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:29:47.249449Z","iopub.execute_input":"2024-03-04T17:29:47.249997Z","iopub.status.idle":"2024-03-04T17:29:47.295302Z","shell.execute_reply.started":"2024-03-04T17:29:47.249962Z","shell.execute_reply":"2024-03-04T17:29:47.294308Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class MSRVTTTEST(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        object_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/resnext101_32x48d-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        val_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_full.txt',\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        self.object_data = memcache(object_data_path)\n        \n        #load validation list and find splits\n        self.split = []\n        \n        with open(val_list) as f:\n            self.val_list = f.readlines()\n        self.val_list = [x.strip() for x in self.val_list]\n        \n        for key in self.val_list:\n        # Check for modalities in the split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False and isinstance(self.scene_data[key], float) == False:\n                self.split.append(key)\n\n    def __getitem__(self, index):\n        key = self.split[index]\n        \n        available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\", \"Face\", \"Scene\", \"Object\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        label = self.label[key][0]\n        \n        #------------Object------------\n        output_data[\"Object\"] = self.object_data[key]\n        if \"Object\" in available_modalities:\n            object_mask = np.ones((self.num_position, 1))\n        else:\n            object_mask = np.zeros((self.num_position, 1))\n        object_pseudo = torch.zeros((text_dim,))\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        audio_pseudo = torch.zeros((text_dim,))\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        rgb_pseudo = torch.zeros((text_dim, ))\n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        ocr_pseudo = torch.zeros((text_dim,))\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        speech_pseudo = torch.zeros((text_dim,))\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        face_pseudo = torch.zeros((text_dim,))\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        scene_pseudo = torch.zeros((text_dim,))\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n            \"Object\": object_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo\n        )\n    \n    def collate_data(self, batch):\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        object_batch = [item[0][\"Object\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        object_masks_batch = [item[2][\"Object\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        object_pseudo_batch = [item[9] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        object_tensor = np.zeros((len(object_batch), 2048))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            object_tensor[i] = object_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n            \"Object\": torch.stack([torch.from_numpy(mask).float() for mask in object_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        object_pseudo = torch.stack(object_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None, \"Object\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        output_data[\"Object\"] = torch.from_numpy(object_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo,\n        )\n    def __len__(self):\n        return len(self.split)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:29:47.296581Z","iopub.execute_input":"2024-03-04T17:29:47.296919Z","iopub.status.idle":"2024-03-04T17:29:47.342811Z","shell.execute_reply.started":"2024-03-04T17:29:47.296895Z","shell.execute_reply":"2024-03-04T17:29:47.341842Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## **Model Architecture**","metadata":{}},{"cell_type":"code","source":"##### Feature Reorganization ####\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViTReorganization(nn.Module):\n    def __init__(self, dim, depth, heads, mlp_dim, pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0., num_position = 16): \n        super().__init__()\n\n        self.audio_embedding = nn.Sequential(\n            nn.LayerNorm(128),\n            nn.Linear(128, dim),\n            nn.LayerNorm(dim)\n        )\n        self.rgb_embedding = nn.Sequential(\n            nn.LayerNorm(2048),\n            nn.Linear(2048, dim),\n            nn.LayerNorm(dim)\n        )\n        self.face_embedding = nn.Sequential(\n            nn.LayerNorm(512),\n            nn.Linear(512, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.speech_embedding = nn.Sequential(\n            nn.LayerNorm(300),\n            nn.Linear(300, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.ocr_embedding = nn.Sequential(\n            nn.LayerNorm(300),\n            nn.Linear(300, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.scene_embedding = nn.Sequential(\n            nn.LayerNorm(2208),\n            nn.Linear(2208, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.object_embedding = nn.Sequential(\n            nn.LayerNorm(2048),\n            nn.Linear(2048, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.embedding = {\n            'Audio': self.audio_embedding,\n            'RGB': self.rgb_embedding,\n            'Face': self.face_embedding,\n            'Speech': self.speech_embedding,\n            'OCR': self.ocr_embedding,\n            'Scene': self.scene_embedding,\n            'Object': self.object_embedding,\n        }\n\n        self.rgb_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n        self.audio_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.face_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n        self.speech_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.ocr_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.scene_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n        self.object_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n\n        self.pos_embedding = {\n            'Audio': self.audio_pos_embedding,\n            'RGB': self.rgb_pos_embedding,\n            'Face': self.face_pos_embedding,\n            'Speech': self.speech_pos_embedding,\n            'OCR': self.ocr_pos_embedding,\n            'Scene': self.scene_pos_embedding,\n            'Object': self.object_pos_embedding,\n        }\n\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.rgb_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) #mlp = multi layer perceptron\n        self.audio_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.face_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.speech_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.ocr_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.scene_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.object_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.transformer = {\n            'RGB': self.rgb_transformer,\n            'Audio': self.audio_transformer,\n            'Face': self.face_transformer,\n            'Speech': self.speech_transformer,\n            'OCR': self.ocr_transformer,\n            'Scene': self.scene_transformer,\n            'Object': self.object_transformer,\n        }\n\n        self.pool = pool #where is pool used???\n        self.to_latent = nn.Identity() #where is this used??\n\n        self.rgb_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n\n        self.audio_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.face_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.speech_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.ocr_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.scene_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.object_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.heads = {\n            'RGB': self.rgb_head,\n            'Audio': self.audio_head,\n            'Face': self.face_head,\n            'Speech': self.speech_head,\n            'OCR': self.ocr_head,\n            'Scene': self.scene_head,\n            'Object': self.object_head,\n        }\n        \n        #fc_layer implemented by us\n        ###\n        self.rgb_fc_layer = nn.Linear(2048, dim)\n        self.audio_fc_layer = nn.Linear(128, dim)\n        self.face_fc_layer = nn.Linear(512, dim)\n        self.speech_fc_layer = nn.Linear(300, dim)\n        self.ocr_fc_layer = nn.Linear(300, dim)\n        self.scene_fc_layer = nn.Linear(2208, dim)\n        self.object_fc_layer = nn.Linear(2048, dim)\n        ###\n    \n    def reorganization(self, feature, position):\n        feature = torch.matmul(\n            feature.transpose(1, 2).contiguous(), position\n        )\n        feature = feature.transpose(1, 2).contiguous()\n        return feature\n\n    def forward(self, rgb_inputs, audio_inputs, face_inputs, speech_inputs, ocr_inputs, scene_inputs, object_inputs): #B is batch size\n        #unsqueeze rgb [B,2048] -> [B,1,2048] which is Fm in the paper\n        rgb_inputs = rgb_inputs.unsqueeze(1)\n        face_inputs = face_inputs.unsqueeze(1)\n        object_inputs = object_inputs.unsqueeze(1)\n\n        # rgb embeddings [B,1,2048] -> [B,1,256]\n        rgb = self.rgb_embedding(rgb_inputs) + self.rgb_pos_embedding\n        audio = self.audio_embedding(audio_inputs) + self.audio_pos_embedding\n        face = self.face_embedding(face_inputs) + self.face_pos_embedding\n        speech = self.speech_embedding(speech_inputs) + self.speech_pos_embedding\n        ocr = self.ocr_embedding(ocr_inputs) + self.ocr_pos_embedding\n        scene = self.scene_embedding(scene_inputs) + self.scene_pos_embedding\n        objectt = self.object_embedding(object_inputs) + self.object_pos_embedding\n\n        # rgb dropout [B,1,256] -> [B,1,256] some values go zero\n        rgb = self.dropout(rgb)\n        audio = self.dropout(audio)\n        face = self.dropout(face)\n        speech = self.dropout(speech)\n        ocr = self.dropout(ocr)\n        scene = self.dropout(scene)\n        objectt = self.dropout(objectt)\n        \n        # rgb transformer [B,1,256] -> [B,1,256]\n        rgb = self.rgb_transformer(rgb)\n        audio = self.audio_transformer(audio)\n        face = self.face_transformer(face)\n        speech = self.speech_transformer(speech)\n        ocr = self.ocr_transformer(ocr)\n        scene = self.scene_transformer(scene)\n        objectt = self.object_transformer(objectt)\n\n        # rgb head [B,1,256] -> [B,1,16] which is Om in the paper\n        rgb = self.rgb_head(rgb)\n        audio = self.audio_head(audio)\n        face = self.face_head(face)\n        speech = self.speech_head(speech)\n        ocr = self.ocr_head(ocr)\n        scene = self.scene_head(scene)\n        objectt = self.object_head(objectt)\n\n        # rgb softmax [B,1,256] -> [B,1,16] which is Om normalized\n        rgb = torch.softmax(rgb, dim = -1)\n        audio = torch.softmax(audio, dim=-1)\n        face = torch.softmax(face, dim=-1)\n        speech = torch.softmax(speech, dim=-1)\n        ocr = torch.softmax(ocr, dim=-1)\n        scene = torch.softmax(scene, dim=-1)\n        objectt = torch.softmax(objectt, dim=-1)\n        \n        # rgb reorganization [1,1,2048] transpose to [1,2048,1]// [1,2048,1]* [B,1,16] = [1,2048,16] transpose to [1,16,2048] which is F'm in the paper\n        rgb = self.reorganization(rgb_inputs, rgb)\n        audio = self.reorganization(audio_inputs, audio)\n        face = self.reorganization(face_inputs, face)\n        speech = self.reorganization(speech_inputs, speech)\n        ocr = self.reorganization(ocr_inputs, ocr)\n        scene = self.reorganization(scene_inputs, scene)\n        objectt = self.reorganization(object_inputs, objectt)\n        \n        #rgb fully connected layer [1,16,2048] -> [1,16,256] which is F^m in the paper \n        #this part of code was missing from the researchers and was implemented by us \n        ###\n        rgb = self.rgb_fc_layer(rgb)\n        audio = self.audio_fc_layer(audio)\n        face = self.face_fc_layer(face)\n        speech = self.speech_fc_layer(speech)\n        ocr = self.ocr_fc_layer(ocr)\n        scene = self.scene_fc_layer(scene)\n        objectt = self.object_fc_layer(objectt)\n        \n        \n        #print(rgb.shape)\n        #print(audio.shape)\n        return rgb, audio, face, speech, ocr, scene, objectt","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:30:01.548631Z","iopub.execute_input":"2024-03-04T17:30:01.549111Z","iopub.status.idle":"2024-03-04T17:30:01.597665Z","shell.execute_reply.started":"2024-03-04T17:30:01.549079Z","shell.execute_reply":"2024-03-04T17:30:01.596397Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pdb\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nimport numpy as np\n\ndef MySoftmax(x, dim=-1, mask=None):\n    maxes = torch.max(x*mask, dim, keepdim=True)[0]\n    x_exp = torch.exp(x-maxes)\n\n    if mask is not None:\n        x_exp_sum = torch.sum(x_exp*mask, dim, keepdim=True)\n        return x_exp * mask / (x_exp_sum + 1e-10)\n    else:\n        x_exp_sum = torch.sum(x_exp, dim, keepdim=True)\n        return x_exp / x_exp_sum\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm_branch(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        # self.fn = fn\n    def forward(self, x):\n        return self.norm(x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention_branch(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x, branch_num):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        qs = qkv[0]\n        ks = qkv[1]\n        vs = qkv[2]\n        per_branch = int((qs.size()[1] - branch_num) // branch_num)\n        outputs = []\n        cls_outputs = []\n        for i in range(branch_num):\n            q = torch.cat((qs[:, i:(i+1), :], qs[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n            k = torch.cat((ks[:, i:(i+1), :], ks[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            k = rearrange(k, 'b n (h d) -> b h n d', h = self.heads)\n            v = torch.cat((vs[:, i:(i+1), :], vs[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            v = rearrange(v, 'b n (h d) -> b h n d', h = self.heads)\n\n            dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n            attn = self.attend(dots)\n            attn = self.dropout(attn)\n            out = torch.matmul(attn, v)\n            out = rearrange(out, 'b h n d -> b n (h d)')\n            cls_outputs.append(out[:,:1])\n            outputs.append(out[:,1:])\n        out = torch.cat(outputs, dim=1)\n        cls_outputs = torch.cat(cls_outputs, dim=1)\n        out = torch.cat((cls_outputs, out), dim=1)\n        # q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        # dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        # attn = self.attend(dots)\n\n        # device = x.get_device()\n        # branch_masks = branch_masks.to(device)\n        # attn_branches = 0\n        # for i in range(branch_masks.size()[1]):\n        #     attn = MySoftmax(dots, dim = -1, mask = branch_masks[:,i])#self.attend(dots)\n        #     attn_branches += attn\n        # attn = self.dropout(attn)\n\n        # out = torch.matmul(attn, v)\n        # out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer_branch(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm_branch(dim),\n                Attention_branch(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                PreNorm_branch(dim),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n    def forward(self, x, branch_num):\n        for pn1, attn, pn2, ff in self.layers:\n            x1 = pn1(x)\n            x = attn(x1, branch_num) + x\n            x1 = pn2(x)\n            x = ff(x1) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., num_position = 16, branch_num = 2):\n        super().__init__()\n        self.mask_ratio = 0.5\n\n        self.audio_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wronng\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        self.rgb_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.face_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.speech_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.ocr_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.scene_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.object_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        per_branch = num_position // branch_num\n        branch_masks = []\n        for i in range(branch_num):\n            branch_mask1 = torch.zeros((1, 1, heads, num_position + branch_num, num_position + branch_num))\n            branch_mask1[:, :, :, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num), (i*per_branch + branch_num) :((i+1)*per_branch + branch_num)] = 1\n            branch_mask1[:, :, :, i, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num)] = 1 # cls mask\n            branch_mask1[:, :, :, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num), i] = 1 # cls mask\n            branch_mask1[:,:,:,i,i] = 1\n            branch_masks.append(branch_mask1)\n        self.branch_masks = torch.cat(branch_masks, dim=1) #[1, branch_num, heads, num_position, num_position]\n        self.branch_num = branch_num\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_position + branch_num, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, branch_num, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer_branch(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, rgb, audio, face, speech, ocr, scene, object_input, rgb_mask, audio_mask, face_mask, speech_mask, ocr_mask, scene_mask, object_mask):\n        rgb = self.rgb_embedding(rgb)\n        audio = self.audio_embedding(audio)\n        face = self.face_embedding(face)\n        speech = self.speech_embedding(speech)\n        ocr = self.ocr_embedding(ocr)\n        scene = self.scene_embedding(scene)\n        objectt = self.object_embedding(object_input)\n        \n        features = rgb * rgb_mask + audio * audio_mask + face * face_mask + speech * speech_mask + ocr * ocr_mask + scene* scene_mask + objectt* object_mask\n        \n        b, n, _ = features.shape\n        x = features + self.pos_embedding[:, self.branch_num:]\n\n        cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)\n        cls_tokens = cls_tokens + self.pos_embedding[:, :self.branch_num]\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        x = self.transformer(x, self.branch_num)\n\n        x = x[:, :self.branch_num]\n\n        x = self.to_latent(x)\n        x = self.mlp_head(x)\n\n        return x\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-03-04T17:30:01.598853Z","iopub.execute_input":"2024-03-04T17:30:01.599143Z","iopub.status.idle":"2024-03-04T17:30:01.641151Z","shell.execute_reply.started":"2024-03-04T17:30:01.599118Z","shell.execute_reply":"2024-03-04T17:30:01.640226Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\nimport pdb\nimport torch\nimport argparse\nimport tqdm\nimport os\nimport numpy as np\nimport torch.nn as nn\nimport random\nimport warnings\nfrom torch.cuda.amp import GradScaler\nimport torch.nn.functional as F\nimport datetime\n\ndef dict_to_cuda(data):\n    for key, value in data.items():\n        data[key] = value.cuda()\n    return data\n\ndef spatialtemporal2tokens(data):\n    b, c, f, h, w = data.size()\n    data = data.view(b, c, f * h * w)\n    data = data.transpose(1, 2).contiguous()\n    return data\n\nclass LabelSmoothLoss(nn.Module):\n    def __init__(self, smoothing=0.0):\n        super(LabelSmoothLoss, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, input, target):\n        log_prob = F.log_softmax(input, dim=-1)\n        weight = input.new_ones(input.size()) * self.smoothing / (input.size(-1) - 1.0)\n        weight.scatter_(-1, target.unsqueeze(-1), (1.0 - self.smoothing))\n        loss = (-weight * log_prob).sum(dim=-1).mean()\n        return loss\n\nclass AlignmentModule(nn.Module):\n    def __init__(self, dim=256):\n        super(AlignmentModule, self).__init__()\n        self.base_vectors = nn.Parameter(torch.randn(1, 128, 256)) #changed from 3086 to 128\n    \n    def forward(self, input):\n        # input [batchsize, n, 256]\n        input = torch.mean(input, dim=1, keepdim=True) # [batch_size, 1, 256]\n        #print(\"input\",input.shape)\n        base_vectors = self.base_vectors.repeat(input.size()[0], 1, 1)\n        #print(\"base_vectors\",base_vectors.shape)\n        sim = torch.mean((base_vectors - input) ** 2, dim=-1)\n        return sim\n\ndef extract_features(unimodal_models, data):\n    outputs = {}\n    for key, value in data.items():\n        outputs[key] = unimodal_models[key](value)\n        if key == 'RGB':\n            outputs[key] = spatialtemporal2tokens(outputs[key])\n    return outputs","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-03-04T17:30:01.642453Z","iopub.execute_input":"2024-03-04T17:30:01.643818Z","iopub.status.idle":"2024-03-04T17:30:01.658831Z","shell.execute_reply.started":"2024-03-04T17:30:01.643774Z","shell.execute_reply":"2024-03-04T17:30:01.657969Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class MaxMarginRankingLoss(nn.Module):\n    def __init__(self, margin=1):\n        super(MaxMarginRankingLoss, self).__init__()\n        self.loss = torch.nn.MarginRankingLoss(margin)\n        self.margin = margin\n\n    def forward(self,x):\n        n = x.size()[0]\n        \n        x1 = torch.diag(x)\n        x1 = x1.unsqueeze(1)\n        x1 = x1.expand(n, n)\n        x1 = x1.contiguous().view(-1,1)\n        x1 = torch.cat((x1,x1),0) \n\n        x2 = x.view(-1,1)\n        x3 = x.transpose(0,1).contiguous().view(-1,1)\n       \n        x2 = torch.cat((x2,x3),0)\n         \n        max_margin = F.relu(self.margin - (x1 - x2))\n        return max_margin.mean()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:30:01.659852Z","iopub.execute_input":"2024-03-04T17:30:01.660119Z","iopub.status.idle":"2024-03-04T17:30:01.672003Z","shell.execute_reply.started":"2024-03-04T17:30:01.660097Z","shell.execute_reply":"2024-03-04T17:30:01.671232Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## **Initializing Everything**","metadata":{}},{"cell_type":"code","source":"torch.backends.cudnn.deterministic = True     #runs the same every time\ntorch.backends.cudnn.benchmark = False \nnp.random.seed(0)\ntorch.manual_seed(0)\nrandom.seed(0)\nwarnings.filterwarnings(\"ignore\")\ndevice = \"cuda\"  # or 'cpu'\ndevice = torch.device(device)\nbatch_size = 128\ntarget_length = 128\nmultimodal_model = ViT(\n        num_classes=text_dim,  #learnable tokens for the feature alignment loss(MSR_VTT= 128)\n        dim=256, #d* -> which is the dim of the common space tokens\n        depth=6, #number of transformer layers\n        heads=8, \n        mlp_dim=512, #hidden dimension of feed-forward layer\n        num_position=16 #k* -> number of tokens in common space\n    )\nmultimodal_model = torch.nn.DataParallel(multimodal_model)\nmultimodal_model = multimodal_model.to(device)\n\n    #feature projection model initialization\nreorganization_module = ViTReorganization(\n        dim=256,\n        depth=6,\n        heads=8,\n        mlp_dim=512, \n)\nreorganization_module = torch.nn.DataParallel(reorganization_module)\nreorganization_module = reorganization_module.to(device)\n\nalignment_model = AlignmentModule()\nalignment_model = torch.nn.DataParallel(alignment_model)\nalignment_model = alignment_model.to(device)\n\nloss_fn = LabelSmoothLoss(smoothing=0.1)\nloss_fn = loss_fn.cuda()\n\noptim = torch.optim.SGD(\n    list(multimodal_model.parameters())+list(reorganization_module.parameters()) + list(alignment_model.parameters()), lr=0.01, momentum=0.9, weight_decay=5e-4\n)\n### maybe change learning rate to lr=0.01\n\ntrain_dataset = MSRVTTTrain(\n        split=\"train\",\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\nvalidation_dataset = MSRVTTValidation(\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=train_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n    )\n\nval_loader = torch.utils.data.DataLoader(\n        validation_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=validation_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n    )\n\nloss_fn = MaxMarginRankingLoss(margin=0.09381161988446174)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T19:10:44.218299Z","iopub.execute_input":"2024-03-04T19:10:44.218681Z","iopub.status.idle":"2024-03-04T19:10:44.799112Z","shell.execute_reply.started":"2024-03-04T19:10:44.218645Z","shell.execute_reply":"2024-03-04T19:10:44.798306Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def verbose(epoch, status, metrics, name='TEST'):\n    print(name + ' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' %\n          (epoch + 1, status,\n           metrics['R1'], metrics['R5'], metrics['R10'],\n           metrics['MR']))\n\n\ndef compute_metric(x):\n    sx = np.sort(-x, axis=1)\n    d = np.diag(-x)\n    d = d[:, np.newaxis]\n    ind = sx - d\n    ind = np.where(ind == 0)\n    ind = ind[1]\n\n    metrics = {}\n    metrics['R1'] = float(np.sum(ind == 0)) / len(ind)\n    metrics['R5'] = float(np.sum(ind < 5)) / len(ind)\n    metrics['R10'] = float(np.sum(ind < 10)) / len(ind)\n    metrics['MR'] = np.median(ind) + 1\n\n    return metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:30:02.702747Z","iopub.execute_input":"2024-03-04T17:30:02.703130Z","iopub.status.idle":"2024-03-04T17:30:02.711478Z","shell.execute_reply.started":"2024-03-04T17:30:02.703097Z","shell.execute_reply":"2024-03-04T17:30:02.710649Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def verbose(epoch, mode, metrics, name=\"TEST\"):\n    r1, r5, r10, r50 = metrics[\"R1\"], metrics[\"R5\"], metrics[\"R10\"], metrics[\"R50\"]\n    msg = f\"{name:s} : {epoch}, R@1: {r1:.1f}\"\n    msg += f\", R@5: {r5:.1f}, R@10 {r10:.1f}, R@50 {r50:.1f}\"\n    msg += f\" MedR: {metrics['MedR']:g}, MeanR: {metrics['MeanR']:.1f}\"\n    print(msg)\n    \ndef v2t_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing captions from the dataset\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n\n    NOTES: We find the closest \"GT caption\" in the style of VSE, which corresponds\n    to finding the rank of the closest relevant caption in embedding space:\n    github.com/ryankiros/visual-semantic-embedding/blob/master/evaluation.py#L52-L56\n    \"\"\"\n    # switch axes of text and video\n    sims = sims.T\n\n    if False:\n        # experiment with toy example\n        sims = np.ones((3, 3))\n        sims[0, 0] = 2\n        sims[1, 1:2] = 2\n        sims[2, :] = 2\n        query_masks = None\n\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_caps = sims.shape\n    dists = -sims\n    caps_per_video = num_caps // num_queries\n    break_ties = \"averaging\"\n\n    MISSING_VAL = 1E8\n    query_ranks = []\n    for ii in range(num_queries):\n        row_dists = dists[ii, :]\n        if query_masks is not None:\n            # Set missing queries to have a distance of infinity.  A missing query\n            # refers to a query position `n` for a video that had less than `n`\n            # captions (for example, a few MSRVTT videos only have 19 queries)\n            row_dists[np.logical_not(query_masks.reshape(-1))] = MISSING_VAL\n\n        # NOTE: Using distance subtraction to perform the ranking is easier to make\n        # deterministic than using argsort, which suffers from the issue of defining\n        # \"stability\" for equal distances.  Example of distance subtraction code:\n        # github.com/antoine77340/Mixture-of-Embedding-Experts/blob/master/train.py\n        sorted_dists = np.sort(row_dists)\n\n        min_rank = np.inf\n        for jj in range(ii * caps_per_video, (ii + 1) * caps_per_video):\n            if row_dists[jj] == MISSING_VAL:\n                # skip rankings of missing captions\n                continue\n            ranks = np.where((sorted_dists - row_dists[jj]) == 0)[0]\n            if break_ties == \"optimistically\":\n                rank = ranks[0]\n            elif break_ties == \"averaging\":\n                # NOTE: If there is more than one caption per video, its possible for the\n                # method to do \"worse than chance\" in the degenerate case when all\n                # similarities are tied.  TODO(Samuel): Address this case.\n                rank = ranks.mean()\n            if rank < min_rank:\n                min_rank = rank\n        query_ranks.append(min_rank)\n    query_ranks = np.array(query_ranks)\n\n    # sanity check against old version of code\n    if False:\n        sorted_dists = np.sort(dists, axis=1)\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        rows_old, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        if rows_old.size > num_queries:\n            _, idx = np.unique(rows_old, return_index=True)\n            cols_old = cols_old[idx]\n        num_diffs = (1 - (cols_old == query_ranks)).sum()\n        msg = f\"new metric doesn't match in {num_diffs} places\"\n        assert np.array_equal(cols_old, query_ranks), msg\n\n        # visualise the distance matrix\n        import sys\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n\n    return cols2metrics(query_ranks, num_queries)\n\ndef cols2metrics(cols, num_queries):\n    metrics = {}\n    metrics[\"R1\"] = 100 * float(np.sum(cols == 0)) / num_queries\n    metrics[\"R5\"] = 100 * float(np.sum(cols < 5)) / num_queries\n    metrics[\"R10\"] = 100 * float(np.sum(cols < 10)) / num_queries\n    metrics[\"R50\"] = 100 * float(np.sum(cols < 50)) / num_queries\n    metrics[\"MedR\"] = np.median(cols) + 1\n    metrics[\"MeanR\"] = np.mean(cols) + 1\n    stats = [metrics[x] for x in (\"R1\", \"R5\", \"R10\")]\n    return metrics\n\ndef t2v_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing queries from the dataset (two videos\n             in MSRVTT only have 19, rather than 20 captions)\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n    \"\"\"\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_vids = sims.shape\n    dists = -sims\n    sorted_dists = np.sort(dists, axis=1)\n\n    if False:\n        import sys\n        import matplotlib\n        from pathlib import Path\n        matplotlib.use(\"Agg\")\n        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n        import ipdb; ipdb.set_trace()\n\n    # The indices are computed such that they slice out the ground truth distances\n    # from the psuedo-rectangular dist matrix\n    queries_per_video = num_queries // num_vids\n    gt_idx = [[np.ravel_multi_index([ii, jj], (num_queries, num_vids))\n              for ii in range(jj * queries_per_video, (jj + 1) * queries_per_video)]\n              for jj in range(num_vids)]\n    gt_idx = np.array(gt_idx)\n    gt_dists = dists.reshape(-1)[gt_idx.reshape(-1)]\n    gt_dists = gt_dists[:, np.newaxis]\n    rows, cols = np.where((sorted_dists - gt_dists) == 0)  # find column position of GT\n\n    # --------------------------------\n    # NOTE: Breaking ties\n    # --------------------------------\n    # We sometimes need to break ties (in general, these should occur extremely rarely,\n    # but there are pathological cases when they can distort the scores, such as when\n    # the similarity matrix is all zeros). Previous implementations (e.g. the t2i\n    # evaluation function used\n    # here: https://github.com/niluthpol/multimodal_vtt/blob/master/evaluation.py and\n    # here: https://github.com/linxd5/VSE_Pytorch/blob/master/evaluation.py#L87) generally\n    # break ties \"optimistically\".  However, if the similarity matrix is constant this\n    # can evaluate to a perfect ranking. A principled option is to average over all\n    # possible partial orderings implied by the ties. See # this paper for a discussion:\n    #    McSherry, Frank, and Marc Najork,\n    #    \"Computing information retrieval performance measures efficiently in the presence\n    #    of tied scores.\" European conference on information retrieval. Springer, Berlin, \n    #    Heidelberg, 2008.\n    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8892&rep=rep1&type=pdf\n\n    # break_ties = \"optimistically\"\n    break_ties = \"averaging\"\n\n    if rows.size > num_queries:\n        assert np.unique(rows).size == num_queries, \"issue in metric evaluation\"\n        if break_ties == \"optimistically\":\n            _, idx = np.unique(rows, return_index=True)\n            cols = cols[idx]\n        elif break_ties == \"averaging\":\n            # fast implementation, based on this code:\n            # https://stackoverflow.com/a/49239335\n            locs = np.argwhere((sorted_dists - gt_dists) == 0)\n\n            # Find the split indices\n            steps = np.diff(locs[:, 0])\n            splits = np.nonzero(steps)[0] + 1\n            splits = np.insert(splits, 0, 0)\n\n            # Compute the result columns\n            summed_cols = np.add.reduceat(locs[:, 1], splits)\n            counts = np.diff(np.append(splits, locs.shape[0]))\n            avg_cols = summed_cols / counts\n            if False:\n                print(\"Running slower code to verify rank averaging across ties\")\n                # slow, but more interpretable version, used for testing\n                avg_cols_slow = [np.mean(cols[rows == idx]) for idx in range(num_queries)]\n                assert np.array_equal(avg_cols, avg_cols_slow), \"slow vs fast difference\"\n                print(\"passed num check\")\n            cols = avg_cols\n\n    msg = \"expected ranks to match queries ({} vs {}) \"\n    if cols.size != num_queries:\n        import ipdb; ipdb.set_trace()\n    assert cols.size == num_queries, msg\n\n    if False:\n        # overload mask to check that we can recover the scores for single-query\n        # retrieval\n        print(\"DEBUGGING MODE\")\n        query_masks = np.zeros_like(query_masks)\n        query_masks[:, 0] = 1  # recover single query score\n\n    if query_masks is not None:\n        # remove invalid queries\n        assert query_masks.size == num_queries, \"invalid query mask shape\"\n        cols = cols[query_masks.reshape(-1).astype(np.bool)]\n        assert cols.size == query_masks.sum(), \"masking was not applied correctly\"\n        # update number of queries to account for those that were missing\n        num_queries = query_masks.sum()\n\n    if False:\n        # sanity check against old logic for square matrices\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        _, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        assert np.array_equal(cols_old, cols), \"new metric doesn't match\"\n\n    return cols2metrics(cols, num_queries)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T17:30:02.712643Z","iopub.execute_input":"2024-03-04T17:30:02.712921Z","iopub.status.idle":"2024-03-04T17:30:02.746381Z","shell.execute_reply.started":"2024-03-04T17:30:02.712898Z","shell.execute_reply":"2024-03-04T17:30:02.745390Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## **Training**","metadata":{}},{"cell_type":"code","source":"import time\nepoch_outputs = []\nBestLoss = float(\"inf\")\nBestEpoch = 0\nprint(\"---------------Start Training---------------\")\nfor epoch_i in range(50):\n    all_truth_outputs = [] \n    all_keys =[]\n    start_time = time.time()\n    dataloaders = {\"train\": train_loader, \"val\":val_loader}\n    split = \"train\"\n    scaler = GradScaler()\n    count = 0\n    count_val = 0\n    total_loss = 0\n    total_loss_val = 0\n    loss = 0\n    multimodal_model.train(split == \"train\")\n    reorganization_module.train(split == \"train\")\n    for (\n        i,\n        (\n            data,\n            labels,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo,\n            key\n        ),\n    ) in enumerate(dataloaders[\"train\"]):\n        \n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n        outputs = data \n        \n        rgb, audio, face, speech, ocr, scene, objectt = reorganization_module(\n        outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"], outputs[\"Object\"]\n        )\n        \n        audio_sim = alignment_model(audio)\n        rgb_sim = alignment_model(rgb)\n        face_sim = alignment_model(face)\n        speech_sim = alignment_model(speech)\n        ocr_sim = alignment_model(ocr)\n        scene_sim = alignment_model(scene)\n        object_sim = alignment_model(objectt)\n        \n        ###first column is for ground truth labels and secod column for pseudo labels\n        outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene, objectt, masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene'], masks['Object']\n        )\n        ###calculate indices\n        audio_indices = torch.sum(masks['Audio'].squeeze(-1), dim=-1) > 0\n        rgb_indices = torch.sum(masks['RGB'].squeeze(-1), dim=-1) > 0\n        face_indices = torch.sum(masks['Face'].squeeze(-1), dim=-1) > 0\n        speech_indices = torch.sum(masks['Speech'].squeeze(-1), dim=-1) > 0\n        ocr_indices = torch.sum(masks['OCR'].squeeze(-1), dim=-1) > 0\n        scene_indices = torch.sum(masks['Scene'].squeeze(-1), dim=-1) > 0\n        object_indices = torch.sum(masks['Object'].squeeze(-1), dim=-1) > 0\n\n        ###calculate ground truth loss\n        truth_outputs = outputs[:,0,:]\n        detached_outputs = truth_outputs.detach().cpu()\n        all_truth_outputs.append(detached_outputs)\n        all_keys.extend(key)\n        \n        truth_labels = labels.mean(dim=1)\n        truth_outputs = F.normalize(truth_outputs, p=2, dim=1)\n        truth_labels = F.normalize(truth_labels, p=2, dim=1)\n        truth_outputs = truth_outputs.to(device)\n        truth_labels = truth_labels.to(device)\n        similarity_matrix = torch.matmul(truth_outputs, truth_labels.t())\n        loss_supervised = loss_fn(similarity_matrix)\n        \n        #check if modalities exist\n        audio_sim = audio_sim[audio_indices]\n        rgb_sim = rgb_sim[rgb_indices]\n        face_sim = face_sim[face_indices]\n        speech_sim = speech_sim[speech_indices]\n        ocr_sim = ocr_sim[ocr_indices]\n        scene_sim = scene_sim[scene_indices]\n        object_sim = object_sim[object_indices]\n        \n        ###calculate alignment loss\n        audio_sim, _ = torch.min(audio_sim, dim=-1)\n        rgb_sim, _ = torch.min(rgb_sim, dim=-1)\n        face_sim, _ = torch.min(face_sim, dim=-1)\n        speech_sim, _ = torch.min(speech_sim, dim=-1)\n        ocr_sim, _ = torch.min(ocr_sim, dim=-1)\n        scene_sim, _ = torch.min(scene_sim, dim=-1)\n        object_sim, _ = torch.min(object_sim, dim=-1)\n        \n        alignment_loss = (torch.sum(rgb_sim)+torch.sum(audio_sim)+torch.sum(face_sim)+torch.sum(speech_sim)+torch.sum(ocr_sim)+torch.sum(scene_sim)+torch.sum(object_sim))/torch.sum(rgb_indices)+(torch.sum(audio_indices)+ torch.sum(face_indices)+ torch.sum(speech_indices)+ torch.sum(ocr_indices)+ torch.sum(scene_indices)+ torch.sum(object_indices))\n\n        confusion_matrix = similarity_matrix.data.cpu().float().numpy()\n        metrics = compute_metric(confusion_matrix)\n        #verbose(epoch_i,batch_size * float(i) / len(dataset),metrics, 'TRAIN')\n        \n        ###calculate pseudo loss\n        pseudo_outputs = outputs[:,1,:]\n        pseudo_outputs = F.normalize(pseudo_outputs, p=2, dim=1)\n        \n        rgb_pseudo = F.normalize(rgb_pseudo, p=2, dim=1)\n        rgb_pseudo = rgb_pseudo.to(device)\n        rgb_cos_sim = F.cosine_similarity(truth_labels, rgb_pseudo, dim=1)\n        audio_pseudo = F.normalize(audio_pseudo, p=2, dim=1)\n        audio_pseudo = audio_pseudo.to(device)\n        ocr_pseudo = F.normalize(ocr_pseudo, p=2, dim=1)\n        ocr_pseudo = ocr_pseudo.to(device)\n        speech_pseudo = F.normalize(speech_pseudo, p=2, dim=1)\n        speech_pseudo = speech_pseudo.to(device)\n        face_pseudo = F.normalize(face_pseudo, p=2, dim=1)\n        face_pseudo = face_pseudo.to(device)\n        scene_pseudo = F.normalize(scene_pseudo, p=2, dim=1)\n        scene_pseudo = scene_pseudo.to(device)\n        object_pseudo = F.normalize(object_pseudo, p=2, dim=1)\n        object_pseudo = object_pseudo.to(device)\n        \n        pseudo_labels = rgb_pseudo\n        max_cos_sim = rgb_cos_sim\n        \n        pseudo_list = [audio_pseudo, ocr_pseudo, scene_pseudo, speech_pseudo, face_pseudo, object_pseudo]\n        for pseudo in pseudo_list:\n            cos_sim = F.cosine_similarity(truth_labels, pseudo, dim=1)\n            truth_table = cos_sim>max_cos_sim\n            pseudo_labels[truth_table] = pseudo[truth_table]\n            max_cos_sim[truth_table] = cos_sim[truth_table]\n        \n\n        pseudo_outputs = pseudo_outputs.to(device)\n        similarity_matrix = torch.matmul(pseudo_outputs, pseudo_labels.t())\n        loss_pseudo = loss_fn(similarity_matrix)\n        ###calculate total loss\n        \n        #loss = alignment_loss*0.001 + loss_supervised\n        loss = alignment_loss*0.001 + loss_supervised + loss_pseudo\n        #loss = loss_supervised + loss_pseudo\n        #loss = loss_supervised\n        \n        optim.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optim)\n        scaler.update()\n\n        total_loss += loss.item() * batch_size\n        #count+= batch_size\n        count += outputs.size()[0]\n    \n    final_truth_outputs = torch.cat(all_truth_outputs, dim=0)\n    final_truth_outputs_np = final_truth_outputs.numpy()\n    \n    if ((epoch_i+1)>49):   \n        #epoch_outputs.append(final_truth_outputs_np)\n        np.save(\"/kaggle/working/pseudo_labels-object.npy\", final_truth_outputs_np)\n        \n        with open(\"/kaggle/working/keys-object.txt\", \"w\") as f:\n            for id in all_keys:\n                f.write(f\"{id}\\n\")\n    all_truth_outputs.clear()\n    all_keys.clear()\n       \n    for (\n        i,\n        (\n            data,\n            labels,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            object_pseudo\n        ),\n    ) in enumerate(dataloaders[\"val\"]):\n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n         \n        \n        with torch.no_grad():\n            outputs = data\n            rgb, audio, face, speech, ocr, scene, objectt = reorganization_module(\n            outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"], outputs[\"Object\"]\n            )\n            outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene, objectt, masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene'], masks['Object']\n            )\n            ###calculate ground truth loss\n            truth_outputs = outputs[:,0,:]\n            truth_labels = labels.mean(dim=1)\n            truth_outputs = F.normalize(truth_outputs, p=2, dim=1)\n            truth_labels = F.normalize(truth_labels, p=2, dim=1)\n            truth_outputs = truth_outputs.to(device)\n            truth_labels = truth_labels.to(device)\n            similarity_matrix = torch.matmul(truth_outputs, truth_labels.t())\n            loss_supervised = loss_fn(similarity_matrix)\n            confusion_matrix = similarity_matrix.data.cpu().float().numpy()\n            #metrics = compute_metric(confusion_matrix)\n            metrics = v2t_metrics(confusion_matrix)\n            verbose(\"Video to Text\",batch_size * float(i) / len(validation_dataset),metrics, 'Validation')\n            total_loss_val += loss_supervised.item() * batch_size\n            #count+= batch_size\n            count_val += outputs.size()[0]\n            \n    if (total_loss_val / float(count_val)) < BestLoss:\n            BestLoss = total_loss_val / float(count_val)\n            BestEpoch = epoch_i+1\n            save = {\n                    \"epoch\": epoch_i+1,\n                    \"model\": multimodal_model.state_dict(),\n                    \"reorganization\": reorganization_module.state_dict(),\n                    \"alignment\": alignment_model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"best_loss\": BestLoss,\n            }\n\n            torch.save(\n                  save,  \"/kaggle/working/best_multimodal_openai_BestLoss_o.pt\"\n            )\n    if (epoch_i>48):\n            BestLoss = total_loss_val / float(count_val)\n            BestEpoch = epoch_i+1\n            save = {\n                    \"epoch\": epoch_i+1,\n                    \"model\": multimodal_model.state_dict(),\n                    \"reorganization\": reorganization_module.state_dict(),\n                    \"alignment\": alignment_model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"best_loss\": BestLoss,\n            }\n\n            torch.save(\n                  save,  \"/kaggle/working/best_multimodal_openai_epoch50_o.pt\"\n            )\n             \n    print(\"Epoch: %02d\" % (epoch_i+1))\n    #print(\"Current Training loss: \", loss.item())\n    print(\"Average Training loss: \", total_loss/float(count))\n    print(\"Average Validation loss: \", total_loss_val/float(count_val))\n    end_time = time.time()\n    dur = end_time-start_time\n    print(f\"Epoch Duration: {dur:.2f} seconds\")\n    print(\"---------------------------------------\")\n#stacked_outputs = np.stack(epoch_outputs)\n#average_output = np.mean(stacked_outputs, axis=0)\n#np.save(\"/kaggle/working/pseudo_labels.npy\", average_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Testing**","metadata":{}},{"cell_type":"code","source":"## Testing###\nmultimodal_model = ViT(\n        num_classes=text_dim,  #learnable tokens for the feature alignment loss(MSR_VTT= 128)\n        dim=256, #d* -> which is the dim of the common space tokens\n        depth=6, #number of transformer layers\n        heads=8, \n        mlp_dim=512, #hidden dimension of feed-forward layer\n        num_position=16 #k* -> number of tokens in common space\n    )\nmultimodal_model = torch.nn.DataParallel(multimodal_model)\nmultimodal_model = multimodal_model.to(device)\n\n    #feature projection model initialization\nreorganization_module = ViTReorganization(\n        dim=256,\n        depth=6,\n        heads=8,\n        mlp_dim=512, \n)\nreorganization_module = torch.nn.DataParallel(reorganization_module)\nreorganization_module = reorganization_module.to(device)\n\ncheckpoint = torch.load(\"/kaggle/working/best_multimodal_openai_epoch50.pt\")\nmultimodal_model.load_state_dict(checkpoint['model'])\nmultimodal_model.eval()\n\nreorganization_module.load_state_dict(checkpoint['reorganization'])\nreorganization_module.eval()\n\ntest_dataset = MSRVTTTEST(\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\ntest_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=test_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False,\n    )\n\nprint(\"---------------Start Testing ---------------\")\nprint(\"Checkpoint from: \" + str(checkpoint['epoch']) + \" epoch  with best loss \" + str(checkpoint['best_loss'])  )\n\naggregated_outputs = []\naggregated_labels = []\n\nfor (\n      i,\n      (\n        data,\n        labels,\n        masks,\n        audio_pseudo,\n        rgb_pseudo,\n        face_pseudo,\n        speech_pseudo,\n        ocr_pseudo,\n        scene_pseudo,\n        object_pseudo\n     ),\n    ) in enumerate(test_loader):\n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n        \n        with torch.no_grad():\n            outputs = data\n            rgb, audio, face, speech, ocr, scene, objectt= reorganization_module(\n            outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"],  outputs[\"Object\"]\n            )\n            outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene, objectt, masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene'],  masks['Object']\n            )\n            #prepare and add to list \n            \n            #outputs = outputs.mean(dim=1).cpu()\n            #truth_outputs = F.normalize(outputs, p=2, dim=1).cpu()\n            \n            truth_outputs = F.normalize(outputs[:,0,:], p=2, dim=1).cpu()\n            truth_labels = F.normalize(labels.mean(dim=1), p=2, dim=1).cpu()\n            aggregated_outputs.append(truth_outputs)\n            aggregated_labels.append(truth_labels)\n            \n# Concatenate all collected outputs and labels\nfinal_outputs = torch.cat(aggregated_outputs, dim=0)\nfinal_labels = torch.cat(aggregated_labels, dim=0)\nsimilarity_matrix = torch.matmul(final_outputs, final_labels.t())\nconfusion_matrix = similarity_matrix.numpy()\nmetrics = v2t_metrics(confusion_matrix)\nverbose(\"Video to Text\",batch_size * float(i) / len(validation_dataset),metrics, 'Test')\nmetrics = t2v_metrics(confusion_matrix)\nverbose(\"Text to Video\",batch_size * float(i) / len(validation_dataset),metrics, 'Test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Draft Results**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised**\n\nCheckpoint from: 20 epoch  with best loss 0.8163572897122601\n\nTest : Video to Text, R@1: **3.0**, R@5: **11.9**, R@10 **20.4** MedR: **51**, MeanR: **111.6**\n\nTest : Text to Video, R@1: **2.7**, R@5: **10.6**, R@10 **19.0** MedR: **44**, MeanR: **97.1**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lalign**\n\nCheckpoint from: 40 epoch  with best loss 0.8537058191975271\n\nTest : Video to Text, R@1: 3.4, R@5: 11.6, R@10 18.0, R@50 49.3 MedR: 52, MeanR: 122.7\n\nTest : Text to Video, R@1: 2.5, R@5: 8.8, R@10 15.6, R@50 42.6 MedR: 69, MeanR: 141.0","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lsudo**\n\nCheckpoint from: 20 epoch  with best loss 0.8174096054918184\n\nTest : Video to Text, R@1: **2.2**, R@5: **12.0**, R@10 **19.9**, R@50 50.5 MedR: **50**, MeanR: **111.4**\n\nTest : Text to Video, R@1: **3.4**, R@5: **11.0**, R@10 **20.5**, R@50 54.2 MedR: **44**, MeanR: **96.7**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised and random labels**\n\nCheckpoint from: 20 epoch  with best loss 0.8158261156457616\n\nTest : Video to Text, R@1: 3.9, R@5: 12.7, R@10 20.4, R@50 54.6 MedR: 41, MeanR: **99.3**\n\nTest : Text to Video, R@1: 2.6, R@5: 9.9, R@10 18.7, R@50 52.8 MedR: 46, MeanR: **99.6**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised, OpenAI random labels**\n\nCheckpoint from: 50 epoch  with best loss 0.8630691288024421\n\nTest : Video to Text, R@1: 4.1, R@5: 13.7, R@10 21.8, R@50 52.5 MedR: 46, MeanR: **105.7**\n\nTest : Text to Video, R@1: 3.4, R@5: 10.3, R@10 17.8, R@50 48.4 MedR: 53, MeanR: **112.0**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.03814773109015517\n\nTest : Video to Text, R@1: **5.5**, R@5: 18.3, R@10 28.1, R@50 64.1 MedR: **30**, MeanR: **71.3**\n\nTest : Text to Video, R@1: **3.7**, R@5: 11.9, R@10 20.7, R@50 55.0 MedR: **42**, MeanR: **91.9**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lalignment, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.03528064442431833\n\nTest : Video to Text, R@1: 5.5, R@5: 17.9, R@10 28.4, R@50 64.4 MedR: 30, MeanR: **71.2**\n\nTest : Text to Video, R@1: 3.7, R@5: 11.9, R@10 20.7, R@50 55.3 MedR: 43, MeanR: **91.7**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lpseudo + Lalignment, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.033720782422643945\n\nTest : Video to Text, R@1: **5.6**, R@5: 19.5, R@10 28.2, R@50 63.7 MedR: 31, MeanR: **70.8**\n\nTest : Text to Video, R@1: **4.1**, R@5: 13.5, R@10 20.7, R@50 58.3 MedR: 36, MeanR: **84.7**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lpseudo + Lalignment, OpenAI random labels, margin=0.1 and OBJECT**\n\nCheckpoint from: 50 epoch  with best loss 0.031087267117237482\n\nTest : Video to Text, R@1: 7.2, R@5: 21.2, R@10 30.7, R@50 68.0 MedR: 25, MeanR: **61.2**\n\nTest : Text to Video, R@1: 3.9, R@5: 14.4, R@10 25.1, R@50 59.7 MedR: 37, MeanR: **77.8**","metadata":{}},{"cell_type":"markdown","source":"## **Final Results**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised**\n\nCheckpoint from: 50 epoch  with best loss 0.03112133844630925\n\nTest : Video to Text, R@1: 7.2, R@5: 21.2, R@10 30.8, R@50 68.0 MedR: 25, MeanR: **61.1**\n\nTest : Text to Video, R@1: 3.8, R@5: 14.4, R@10 24.8, R@50 59.6 MedR: 37, MeanR: **78.0**\n\n","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lalignment**\n\nCheckpoint from: 50 epoch  with best loss 0.031087267117237482\n\nTest : Video to Text, R@1: 7.2, R@5: 21.2, R@10 30.7, R@50 68.0 MedR: 25, MeanR: **61.2**\n\nTest : Text to Video, R@1: 3.9, R@5: 14.4, R@10 25.1, R@50 59.7 MedR: 37, MeanR: **77.8**\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lpseudo + Lalignment**\n\nCheckpoint from: 50 epoch  with best loss 0.031087267117237482\n\nTest : Video to Text, R@1: 7.2, R@5: 21.2, R@10 30.7, R@50 68.0 MedR: 25, MeanR: **61.2**\n\nTest : Text to Video, R@1: 3.9, R@5: 14.4, R@10 25.1, R@50 59.7 MedR: 37, MeanR: **77.8**\n","metadata":{}}]}