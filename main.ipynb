{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7408740,"sourceType":"datasetVersion","datasetId":4309021},{"sourceId":7705966,"sourceType":"datasetVersion","datasetId":4498963}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-27T17:18:23.630913Z","iopub.execute_input":"2024-02-27T17:18:23.631235Z","iopub.status.idle":"2024-02-27T17:18:24.682270Z","shell.execute_reply.started":"2024-02-27T17:18:23.631206Z","shell.execute_reply":"2024-02-27T17:18:24.681325Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_jsfusion.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_miech.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_jsfusion.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/raw-captions.pkl\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_full.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_miech.txt\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_r2p1d_30fps_256px_stride32_offset0_inner_stride1/r2p1d-ig65m-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/audio_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/flow_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/facefeats-clone.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/antoine/resnet_features.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/resnext101_32x48d-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_i3d_25fps_256px_stride25_offset0_inner_stride1/i3d-avg.pickle\n/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle\n/kaggle/input/pseudo-labels-rgb/pseudo_labels (1).npy\n/kaggle/input/pseudo-labels-rgb/keys.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install zsvision","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:24.684692Z","iopub.execute_input":"2024-02-27T17:18:24.685243Z","iopub.status.idle":"2024-02-27T17:18:39.191280Z","shell.execute_reply.started":"2024-02-27T17:18:24.685207Z","shell.execute_reply":"2024-02-27T17:18:39.190127Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting zsvision\n  Obtaining dependency information for zsvision from https://files.pythonhosted.org/packages/5b/64/ed80e275858e8e88fc8c46e73d9f15c8c660cd46c1e996d50c18453e3172/zsvision-0.7.12-py3-none-any.whl.metadata\n  Downloading zsvision-0.7.12-py3-none-any.whl.metadata (897 bytes)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.24.3)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.0.5)\nRequirement already satisfied: msgpack-numpy in /opt/conda/lib/python3.10/site-packages (from zsvision) (0.4.8)\nRequirement already satisfied: typeguard in /opt/conda/lib/python3.10/site-packages (from zsvision) (2.13.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from zsvision) (1.11.4)\nCollecting mergedeep (from zsvision)\n  Obtaining dependency information for mergedeep from https://files.pythonhosted.org/packages/2c/19/04f9b178c2d8a15b076c8b5140708fa6ffc5601fb6f1e975537072df5b2a/mergedeep-1.3.4-py3-none-any.whl.metadata\n  Downloading mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: humanize in /opt/conda/lib/python3.10/site-packages (from zsvision) (4.9.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from zsvision) (3.7.4)\nCollecting beartype>=0.3.2 (from zsvision)\n  Obtaining dependency information for beartype>=0.3.2 from https://files.pythonhosted.org/packages/81/4a/97ea8a5afb289a25ae7db3b3ef68f0aad892bc1756be94565154877b173e/beartype-0.17.2-py3-none-any.whl.metadata\n  Downloading beartype-0.17.2-py3-none-any.whl.metadata (30 kB)\nCollecting hickle>=4.0.0 (from zsvision)\n  Obtaining dependency information for hickle>=4.0.0 from https://files.pythonhosted.org/packages/a0/fc/f91e9b1f6f5581b09c91339832a03b4e63444eb1a3565c9d5bad4abaebdd/hickle-5.0.2-py3-none-any.whl.metadata\n  Downloading hickle-5.0.2-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from zsvision) (6.0.1)\nRequirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from hickle>=4.0.0->zsvision) (3.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->zsvision) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->zsvision) (1.16.0)\nDownloading zsvision-0.7.12-py3-none-any.whl (25 kB)\nDownloading beartype-0.17.2-py3-none-any.whl (872 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m872.4/872.4 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hickle-5.0.2-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\nInstalling collected packages: mergedeep, beartype, hickle, zsvision\nSuccessfully installed beartype-0.17.2 hickle-5.0.2 mergedeep-1.3.4 zsvision-0.7.12\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfrom zsvision.zs_utils import memcache, concat_features\n\n#file_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle'\n\n#rgb = memcache(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:39.192719Z","iopub.execute_input":"2024-02-27T17:18:39.193025Z","iopub.status.idle":"2024-02-27T17:18:39.511293Z","shell.execute_reply.started":"2024-02-27T17:18:39.192998Z","shell.execute_reply":"2024-02-27T17:18:39.510301Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_full.txt') as f:\n    train_list = f.readlines()\ntrain_list = [x.strip() for x in train_list]","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:39.512571Z","iopub.execute_input":"2024-02-27T17:18:39.512899Z","iopub.status.idle":"2024-02-27T17:18:39.529685Z","shell.execute_reply.started":"2024-02-27T17:18:39.512871Z","shell.execute_reply":"2024-02-27T17:18:39.528890Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(train_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:39.532124Z","iopub.execute_input":"2024-02-27T17:18:39.532444Z","iopub.status.idle":"2024-02-27T17:18:39.539429Z","shell.execute_reply.started":"2024-02-27T17:18:39.532418Z","shell.execute_reply":"2024-02-27T17:18:39.538435Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"6513"},"metadata":{}}]},{"cell_type":"code","source":"#print(rgb[\"video1\"].shape)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:39.540583Z","iopub.execute_input":"2024-02-27T17:18:39.540884Z","iopub.status.idle":"2024-02-27T17:18:39.548683Z","shell.execute_reply.started":"2024-02-27T17:18:39.540860Z","shell.execute_reply":"2024-02-27T17:18:39.547792Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"file_path_audio = \"/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle\"\nspeech = memcache(file_path_audio)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:39.549925Z","iopub.execute_input":"2024-02-27T17:18:39.550570Z","iopub.status.idle":"2024-02-27T17:18:40.113177Z","shell.execute_reply.started":"2024-02-27T17:18:39.550537Z","shell.execute_reply":"2024-02-27T17:18:40.112273Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"loading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle (31102aa59d54) [I/O: 0.5s] [deserialisation: 0.1s] [Total: 0.6s]\n","output_type":"stream"}]},{"cell_type":"code","source":"largest_dim = 0\naa=0\nfor key, value in speech.items():\n    # Check if the item is a tensor and not a float\n    if (isinstance(value, float)!=True):\n        # Update largest_dim if this tensor's max dimension is larger\n        a,current_max_dim = value.shape  # Largest dimension of the current tensor\n        largest_dim = max(largest_dim, current_max_dim)\n        aa = max(aa,a)\n\nprint(\"The largest dimension found in the matrices is:\", largest_dim, aa)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:40.114544Z","iopub.execute_input":"2024-02-27T17:18:40.114852Z","iopub.status.idle":"2024-02-27T17:18:40.134113Z","shell.execute_reply.started":"2024-02-27T17:18:40.114827Z","shell.execute_reply":"2024-02-27T17:18:40.133175Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The largest dimension found in the matrices is: 2208 1\n","output_type":"stream"}]},{"cell_type":"code","source":"rgb_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle')\naudio_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle')\nspeech_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle')\nocr_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle')\nscene_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle')\nface_data = memcache('/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:40.135281Z","iopub.execute_input":"2024-02-27T17:18:40.135726Z","iopub.status.idle":"2024-02-27T17:18:42.646560Z","shell.execute_reply.started":"2024-02-27T17:18:40.135693Z","shell.execute_reply":"2024-02-27T17:18:42.645495Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"loading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle (31102aa59d54) [I/O: 0.3s] [deserialisation: 0.0s] [Total: 0.4s]\nloading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle (31102aa59d54) [I/O: 0.5s] [deserialisation: 0.1s] [Total: 0.6s]\nloading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle (31102aa59d54) [I/O: 0.9s] [deserialisation: 0.1s] [Total: 1.0s]\nloading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle (31102aa59d54) [I/O: 0.2s] [deserialisation: 0.0s] [Total: 0.2s]\nloading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle (31102aa59d54) [I/O: 0.2s] [deserialisation: 0.0s] [Total: 0.2s]\n","output_type":"stream"}]},{"cell_type":"code","source":"text_feats = memcache(\"/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:18:42.647831Z","iopub.execute_input":"2024-02-27T17:18:42.648102Z","iopub.status.idle":"2024-02-27T17:19:31.246703Z","shell.execute_reply.started":"2024-02-27T17:18:42.648080Z","shell.execute_reply":"2024-02-27T17:19:31.245777Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"loading data from /kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle (31102aa59d54) [I/O: 43.6s] [deserialisation: 4.7s] [Total: 48.6s]\n","output_type":"stream"}]},{"cell_type":"code","source":"text_feats[\"video20\"][1].shape","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:19:31.248032Z","iopub.execute_input":"2024-02-27T17:19:31.248333Z","iopub.status.idle":"2024-02-27T17:19:31.254444Z","shell.execute_reply.started":"2024-02-27T17:19:31.248308Z","shell.execute_reply":"2024-02-27T17:19:31.253508Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(10, 768)"},"metadata":{}}]},{"cell_type":"code","source":"split1_keys = []\n\nwith open('/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_full.txt') as f:\n    train_list = f.readlines()\ntrain_list = [x.strip() for x in train_list]\nfor key in train_list:\n    # Check for modalities in the first split\n    if isinstance(audio_data[key], float) == False and isinstance(ocr_data[key], float) == False and isinstance(speech_data[key], float) == False and isinstance(scene_data[key], float) == False:\n        split1_keys.append(key)\n\nprint(f\"First split has {len(split1_keys)} samples.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:19:31.255553Z","iopub.execute_input":"2024-02-27T17:19:31.255828Z","iopub.status.idle":"2024-02-27T17:19:31.270591Z","shell.execute_reply.started":"2024-02-27T17:19:31.255804Z","shell.execute_reply":"2024-02-27T17:19:31.269719Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"First split has 127 samples.\n","output_type":"stream"}]},{"cell_type":"code","source":"split1_keys[124]","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:19:31.271890Z","iopub.execute_input":"2024-02-27T17:19:31.272252Z","iopub.status.idle":"2024-02-27T17:19:31.278129Z","shell.execute_reply.started":"2024-02-27T17:19:31.272221Z","shell.execute_reply":"2024-02-27T17:19:31.277315Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'video6990'"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Dataloaders**","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport random\nimport numpy as np\nimport os\nimport random\ntext_dim = 768\nclass MSRVTTTrain(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        train_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/train_list_full.txt',\n        pseudo_labels_path = \"/kaggle/input/pseudo-labels-rgb/pseudo_labels.npy\",\n        keys_path = \"/kaggle/input/pseudo-labels-rgb/keys.txt\",\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        \n        #load pseudo labels\n        self.pseudo_labels = np.load(pseudo_labels_path)\n        self.keys = []\n        with open(keys_path, \"r\") as f:\n            for line in f:\n                self.keys.append(line.strip())\n        \n        #load train list and find splits\n        self.split1 = []\n        self.split2 = []\n        \n        with open(train_list) as f:\n            self.train_list = f.readlines()\n        self.train_list = [x.strip() for x in self.train_list]\n        \n        for key in self.train_list:\n        # Check for modalities in the first split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False:\n                self.split1.append(key)\n            else:\n                if isinstance(self.scene_data[key], float) == False :\n                    self.split2.append(key)\n        \n\n    def __getitem__(self, index):\n        key = self.train_list[index]\n        #available_modalities = [\"RGB\"]\n        if key in self.split1:\n            available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\"]\n        elif key in self.split2:\n            available_modalities = [\"RGB\", \"Face\", \"Scene\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        \n        #pick random label each time\n        r = random.randint(0, len(self.label[key])-1)\n        label = self.label[key][r]\n        \n        #pick the first label\n        #label = self.label[key][0]\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        audio_pseudo = torch.zeros((text_dim,))\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        index = self.keys.index(key)\n        rgb_pseudo = torch.Tensor(self.pseudo_labels[index])\n        \n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        ocr_pseudo = torch.zeros((text_dim,))\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        speech_pseudo = torch.zeros((text_dim,))\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        face_pseudo = torch.zeros((text_dim,))\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        scene_pseudo = torch.zeros((text_dim,))\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            key\n        )\n    \n    def collate_data(self, batch):\n        keys = [item[-1] for item in batch]\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            keys\n        )\n    \n    def __len__(self):\n        return len(self.train_list)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:37:23.806156Z","iopub.execute_input":"2024-02-27T17:37:23.806601Z","iopub.status.idle":"2024-02-27T17:37:23.853751Z","shell.execute_reply.started":"2024-02-27T17:37:23.806570Z","shell.execute_reply":"2024-02-27T17:37:23.852775Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class MSRVTTValidation(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        val_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/val_list_full.txt',\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        \n        #load validation list and find splits\n        self.split = []\n        \n        with open(val_list) as f:\n            self.val_list = f.readlines()\n        self.val_list = [x.strip() for x in self.val_list]\n        \n        for key in self.val_list:\n        # Check for modalities in the split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False and isinstance(self.scene_data[key], float) == False:\n                self.split.append(key)\n\n    def __getitem__(self, index):\n        key = self.split[index]\n        \n        available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\", \"Face\", \"Scene\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        label = self.label[key][0]\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        audio_pseudo = torch.zeros((text_dim,))\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        rgb_pseudo = torch.zeros((text_dim, ))\n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        ocr_pseudo = torch.zeros((text_dim,))\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        speech_pseudo = torch.zeros((text_dim,))\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        face_pseudo = torch.zeros((text_dim,))\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        scene_pseudo = torch.zeros((text_dim,))\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n        )\n    \n    def collate_data(self, batch):\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n        )\n    \n    def __len__(self):\n        return len(self.split)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:19:34.750791Z","iopub.execute_input":"2024-02-27T17:19:34.751348Z","iopub.status.idle":"2024-02-27T17:19:34.794406Z","shell.execute_reply.started":"2024-02-27T17:19:34.751311Z","shell.execute_reply":"2024-02-27T17:19:34.793233Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MSRVTTTEST(Dataset):\n    def __init__(\n        self,\n        split=\"train\",\n        audio_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_audio_feats/Audio_MSRVTT_new.pickle',\n        label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT_openAIGPT.pickle',\n        rgb_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_imagenet_25fps_256px_stride1_offset0/senet154-avg.pickle',\n        speech_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_speech/speech-w2v.pickle',\n        ocr_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_ocr_feats/ocr-raw.pickle',\n        scene_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_scene_25fps_256px_stride1_offset0/densenet161-avg.pickle',\n        face_data_path = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_face_feats/facefeats-avg.pickle',\n        val_list = '/kaggle/input/msr-vtt-features-expert/structured-symlinks/test_list_full.txt',\n        num_position=16,   \n        max_words = 30,\n    ):\n        \n        self.num_position = num_position\n        self.rgb_data = memcache(rgb_data_path)\n        self.audio_data = memcache(audio_data_path)\n        self.label = memcache(label_data_path)\n        self.max_words = max_words\n        self.speech_data = memcache(speech_data_path)\n        self.ocr_data = memcache(ocr_data_path)\n        self.scene_data = memcache(scene_data_path)\n        self.face_data = memcache(face_data_path)\n        \n        #load validation list and find splits\n        self.split = []\n        \n        with open(val_list) as f:\n            self.val_list = f.readlines()\n        self.val_list = [x.strip() for x in self.val_list]\n        \n        for key in self.val_list:\n        # Check for modalities in the split\n            if isinstance(self.audio_data[key], float) == False and isinstance(self.ocr_data[key], float) == False and isinstance(self.speech_data[key], float) == False and isinstance(self.scene_data[key], float) == False:\n                self.split.append(key)\n\n    def __getitem__(self, index):\n        key = self.split[index]\n        \n        available_modalities = [\"RGB\", \"Audio\", \"OCR\", \"Speech\", \"Face\", \"Scene\"]\n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        label = self.label[key][0]\n        \n        #------------Audio------------\n        output_data[\"Audio\"] = self.audio_data[key]\n        if \"Audio\" in available_modalities:\n            audio_mask = np.ones((self.num_position, 1))\n        else:\n            audio_mask = np.zeros((self.num_position, 1))\n        audio_pseudo = torch.zeros((text_dim,))\n            \n        #------------RGB------------\n        output_data[\"RGB\"] = self.rgb_data[key]\n        if \"RGB\" in available_modalities:\n            rgb_mask = np.ones((self.num_position, 1))\n        else: \n            rgb_mask = np.zeros((self.num_position, 1))\n        rgb_pseudo = torch.zeros((text_dim, ))\n        \n        #------------OCR------------\n        output_data[\"OCR\"] = self.ocr_data[key]\n        if (isinstance(output_data[\"OCR\"],float) != True) and \"OCR\" in available_modalities:\n            ocr_mask = np.ones((self.num_position, 1))\n        else:\n            ocr_mask = np.zeros((self.num_position, 1))\n        ocr_pseudo = torch.zeros((text_dim,))\n        \n        #------------Speech------------\n        output_data[\"Speech\"] = self.speech_data[key]\n        if (isinstance(output_data[\"Speech\"],float) != True) and \"Speech\" in available_modalities:\n            speech_mask = np.ones((self.num_position, 1))\n        else:\n            speech_mask = np.zeros((self.num_position, 1))\n        speech_pseudo = torch.zeros((text_dim,))\n        \n        #------------Face------------\n        \n        output_data[\"Face\"] = self.face_data[key]\n        if (isinstance(output_data[\"Face\"],float) != True) and \"Speech\" in available_modalities:\n            face_mask = np.ones((self.num_position, 1))\n        else:\n            face_mask = np.zeros((self.num_position, 1))\n        face_pseudo = torch.zeros((text_dim,))\n        \n        #------------Scene------------\n        output_data[\"Scene\"] = self.scene_data[key]\n        if \"Scene\" in available_modalities:\n            scene_mask = np.ones((self.num_position, 1))\n        else:\n            scene_mask = np.zeros((self.num_position, 1))\n        scene_pseudo = torch.zeros((text_dim,))\n        \n        masks = {\n            \"RGB\": rgb_mask.astype(np.float32),\n            \"Audio\": audio_mask.astype(np.float32),\n            \"OCR\": ocr_mask.astype(np.float32),\n            \"Speech\": speech_mask.astype(np.float32),\n            \"Face\": face_mask.astype(np.float32),\n            \"Scene\": scene_mask.astype(np.float32),\n        }\n\n        \n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n        )\n    \n    def collate_data(self, batch):\n        audio_batch = [item[0][\"Audio\"] for item in batch]\n        rgb_batch = [item[0][\"RGB\"] for item in batch]\n        face_batch = [item[0][\"Face\"] for item in batch]\n        speech_batch = [item[0][\"Speech\"] for item in batch]\n        ocr_batch = [item[0][\"OCR\"] for item in batch]\n        scene_batch = [item[0][\"Scene\"] for item in batch]\n        \n        label_batch = [item[1] for item in batch]\n        \n        rgb_masks_batch = [item[2][\"RGB\"] for item in batch]\n        audio_masks_batch = [item[2][\"Audio\"] for item in batch]\n        face_masks_batch = [item[2][\"Face\"] for item in batch]\n        speech_masks_batch = [item[2][\"Speech\"] for item in batch]\n        ocr_masks_batch = [item[2][\"OCR\"] for item in batch]\n        scene_masks_batch = [item[2][\"Scene\"] for item in batch]\n        \n        audio_pseudo_batch = [item[3] for item in batch]\n        rgb_pseudo_batch = [item[4] for item in batch]\n        face_pseudo_batch = [item[5] for item in batch]\n        speech_pseudo_batch = [item[6] for item in batch]\n        ocr_pseudo_batch = [item[7] for item in batch]\n        scene_pseudo_batch = [item[8] for item in batch]\n        \n        video_tensor = np.zeros((len(rgb_batch), 2048))\n        audio_tensor = np.zeros((len(audio_batch), self.max_words,128))\n        text_tensor = np.zeros((len(label_batch), self.max_words, text_dim))\n        face_tensor = np.zeros((len(face_batch), 512))\n        speech_tensor = np.zeros((len(speech_batch), self.max_words, 300))\n        ocr_tensor = np.zeros((len(ocr_batch), self.max_words, 300))\n        scene_tensor = np.zeros((len(scene_batch), 1, 2208))\n        \n        for i in range(len(label_batch)):\n\n            video_tensor[i] = rgb_batch[i] \n            \n            scene_tensor[i] = scene_batch[i]\n            \n            if (isinstance(face_batch[i], float) !=True):\n                face_tensor[i] = face_batch[i]\n\n            la = len(audio_batch[i])\n            audio_tensor[i,:min(la,self.max_words), :] = audio_batch[i][:min(self.max_words,la)]\n\n            lt = len(label_batch[i])\n            text_tensor[i,:min(lt,self.max_words), :] = label_batch[i][:min(self.max_words,lt)]\n            \n            if (isinstance(ocr_batch[i], float) !=True):\n                lo = len(ocr_batch[i])\n                ocr_tensor[i,:min(lo,self.max_words), :] = ocr_batch[i][:min(self.max_words,lo)]\n\n            if (isinstance(speech_batch[i], float) !=True):\n                ls = len(speech_batch[i])\n                speech_tensor[i,:min(ls,self.max_words), :] = speech_batch[i][:min(self.max_words,ls)]\n                      \n        masks = {\n            \"RGB\": torch.stack([torch.from_numpy(mask).float() for mask in rgb_masks_batch]),\n            \"Audio\": torch.stack([torch.from_numpy(mask).float() for mask in audio_masks_batch]),\n            \"OCR\": torch.stack([torch.from_numpy(mask).float() for mask in ocr_masks_batch]),\n            \"Speech\": torch.stack([torch.from_numpy(mask).float() for mask in speech_masks_batch]),\n            \"Face\": torch.stack([torch.from_numpy(mask).float() for mask in face_masks_batch]),\n            \"Scene\": torch.stack([torch.from_numpy(mask).float() for mask in scene_masks_batch]),\n        }\n        \n        audio_pseudo = torch.stack(audio_pseudo_batch)\n        rgb_pseudo = torch.stack(rgb_pseudo_batch)\n        speech_pseudo = torch.stack(speech_pseudo_batch)\n        face_pseudo = torch.stack(face_pseudo_batch)\n        ocr_pseudo = torch.stack(ocr_pseudo_batch)\n        scene_pseudo = torch.stack(scene_pseudo_batch)\n        \n        output_data = {\"Audio\": None, \"RGB\": None, \"OCR\": None, \"Speech\": None, \"Face\": None, \"Scene\": None}\n        output_data[\"Audio\"] = torch.from_numpy(audio_tensor).float()\n        output_data[\"RGB\"] = torch.from_numpy(video_tensor).float()\n        output_data[\"OCR\"] = torch.from_numpy(ocr_tensor).float()\n        output_data[\"Speech\"] = torch.from_numpy(speech_tensor).float()\n        output_data[\"Face\"] = torch.from_numpy(face_tensor).float()\n        output_data[\"Scene\"] = torch.from_numpy(scene_tensor).float()\n        \n        label = torch.from_numpy(text_tensor).float()\n\n        return (\n            output_data,\n            label,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n        )\n    \n    def __len__(self):\n        return len(self.split)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T17:19:34.795875Z","iopub.execute_input":"2024-02-27T17:19:34.796172Z","iopub.status.idle":"2024-02-27T17:19:34.838143Z","shell.execute_reply.started":"2024-02-27T17:19:34.796148Z","shell.execute_reply":"2024-02-27T17:19:34.837215Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# all imports\n!pip install einops\nimport torch\nfrom torch import nn\nimport pdb\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:34.839226Z","iopub.execute_input":"2024-02-27T17:19:34.839518Z","iopub.status.idle":"2024-02-27T17:19:49.101002Z","shell.execute_reply.started":"2024-02-27T17:19:34.839494Z","shell.execute_reply":"2024-02-27T17:19:49.099882Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting einops\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"##### Feature Reorganization ####\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViTReorganization(nn.Module):\n    def __init__(self, dim, depth, heads, mlp_dim, pool = 'cls', dim_head = 64, dropout = 0., emb_dropout = 0., num_position = 16): \n        super().__init__()\n\n        self.audio_embedding = nn.Sequential(\n            nn.LayerNorm(128),\n            nn.Linear(128, dim),\n            nn.LayerNorm(dim)\n        )\n        self.rgb_embedding = nn.Sequential(\n            nn.LayerNorm(2048),\n            nn.Linear(2048, dim),\n            nn.LayerNorm(dim)\n        )\n        self.face_embedding = nn.Sequential(\n            nn.LayerNorm(512),\n            nn.Linear(512, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.speech_embedding = nn.Sequential(\n            nn.LayerNorm(300),\n            nn.Linear(300, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.ocr_embedding = nn.Sequential(\n            nn.LayerNorm(300),\n            nn.Linear(300, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.scene_embedding = nn.Sequential(\n            nn.LayerNorm(2208),\n            nn.Linear(2208, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.embedding = {\n            'Audio': self.audio_embedding,\n            'RGB': self.rgb_embedding,\n            'Face': self.face_embedding,\n            'Speech': self.speech_embedding,\n            'OCR': self.ocr_embedding,\n            'Scene': self.scene_embedding,\n        }\n\n        self.rgb_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n        self.audio_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.face_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n        self.speech_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.ocr_pos_embedding = nn.Parameter(torch.randn(1,30, dim))\n        self.scene_pos_embedding = nn.Parameter(torch.randn(1,1, dim))\n\n        self.pos_embedding = {\n            'Audio': self.audio_pos_embedding,\n            'RGB': self.rgb_pos_embedding,\n            'Face': self.face_pos_embedding,\n            'Speech': self.speech_pos_embedding,\n            'OCR': self.ocr_pos_embedding,\n            'Scene': self.scene_pos_embedding,\n        }\n\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.rgb_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout) #mlp = multi layer perceptron\n        self.audio_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.face_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.speech_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.ocr_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n        self.scene_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.transformer = {\n            'RGB': self.rgb_transformer,\n            'Audio': self.audio_transformer,\n            'Face': self.face_transformer,\n            'Speech': self.speech_transformer,\n            'OCR': self.ocr_transformer,\n            'Scene': self.scene_transformer,\n        }\n\n        self.pool = pool #where is pool used???\n        self.to_latent = nn.Identity() #where is this used??\n\n        self.rgb_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n\n        self.audio_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.face_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.speech_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.ocr_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.scene_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_position)\n        )\n        \n        self.heads = {\n            'RGB': self.rgb_head,\n            'Audio': self.audio_head,\n            'Face': self.face_head,\n            'Speech': self.speech_head,\n            'OCR': self.ocr_head,\n            'Scene': self.scene_head,\n        }\n        \n        #fc_layer implemented by us\n        ###\n        self.rgb_fc_layer = nn.Linear(2048, dim)\n        self.audio_fc_layer = nn.Linear(128, dim)\n        self.face_fc_layer = nn.Linear(512, dim)\n        self.speech_fc_layer = nn.Linear(300, dim)\n        self.ocr_fc_layer = nn.Linear(300, dim)\n        self.scene_fc_layer = nn.Linear(2208, dim)\n        ###\n    \n    def reorganization(self, feature, position):\n        feature = torch.matmul(\n            feature.transpose(1, 2).contiguous(), position\n        )\n        feature = feature.transpose(1, 2).contiguous()\n        return feature\n\n    def forward(self, rgb_inputs, audio_inputs, face_inputs, speech_inputs, ocr_inputs, scene_inputs): #B is batch size\n        #unsqueeze rgb [B,2048] -> [B,1,2048] which is Fm in the paper\n        rgb_inputs = rgb_inputs.unsqueeze(1)\n        face_inputs = face_inputs.unsqueeze(1)\n\n        # rgb embeddings [B,1,2048] -> [B,1,256]\n        rgb = self.rgb_embedding(rgb_inputs) + self.rgb_pos_embedding\n        audio = self.audio_embedding(audio_inputs) + self.audio_pos_embedding\n        face = self.face_embedding(face_inputs) + self.face_pos_embedding\n        speech = self.speech_embedding(speech_inputs) + self.speech_pos_embedding\n        ocr = self.ocr_embedding(ocr_inputs) + self.ocr_pos_embedding\n        scene = self.scene_embedding(scene_inputs) + self.scene_pos_embedding\n\n        # rgb dropout [B,1,256] -> [B,1,256] some values go zero\n        rgb = self.dropout(rgb)\n        audio = self.dropout(audio)\n        face = self.dropout(face)\n        speech = self.dropout(speech)\n        ocr = self.dropout(ocr)\n        scene = self.dropout(scene)\n        \n        # rgb transformer [B,1,256] -> [B,1,256]\n        rgb = self.rgb_transformer(rgb)\n        audio = self.audio_transformer(audio)\n        face = self.face_transformer(face)\n        speech = self.speech_transformer(speech)\n        ocr = self.ocr_transformer(ocr)\n        scene = self.scene_transformer(scene)\n\n        # rgb head [B,1,256] -> [B,1,16] which is Om in the paper\n        rgb = self.rgb_head(rgb)\n        audio = self.audio_head(audio)\n        face = self.face_head(face)\n        speech = self.speech_head(speech)\n        ocr = self.ocr_head(ocr)\n        scene = self.scene_head(scene)\n\n        # rgb softmax [B,1,256] -> [B,1,16] which is Om normalized\n        rgb = torch.softmax(rgb, dim = -1)\n        audio = torch.softmax(audio, dim=-1)\n        face = torch.softmax(face, dim=-1)\n        speech = torch.softmax(speech, dim=-1)\n        ocr = torch.softmax(ocr, dim=-1)\n        scene = torch.softmax(scene, dim=-1)\n        \n        # rgb reorganization [1,1,2048] transpose to [1,2048,1]// [1,2048,1]* [B,1,16] = [1,2048,16] transpose to [1,16,2048] which is F'm in the paper\n        rgb = self.reorganization(rgb_inputs, rgb)\n        audio = self.reorganization(audio_inputs, audio)\n        face = self.reorganization(face_inputs, face)\n        speech = self.reorganization(speech_inputs, speech)\n        ocr = self.reorganization(ocr_inputs, ocr)\n        scene = self.reorganization(scene_inputs, scene)\n        \n        #rgb fully connected layer [1,16,2048] -> [1,16,256] which is F^m in the paper \n        #this part of code was missing from the researchers and was implemented by us \n        ###\n        rgb = self.rgb_fc_layer(rgb)\n        audio = self.audio_fc_layer(audio)\n        face = self.face_fc_layer(face)\n        speech = self.speech_fc_layer(speech)\n        ocr = self.ocr_fc_layer(ocr)\n        scene = self.scene_fc_layer(scene)\n        \n        \n        #print(rgb.shape)\n        #print(audio.shape)\n        return rgb, audio, face, speech, ocr, scene","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:49.102844Z","iopub.execute_input":"2024-02-27T17:19:49.103328Z","iopub.status.idle":"2024-02-27T17:19:49.149447Z","shell.execute_reply.started":"2024-02-27T17:19:49.103300Z","shell.execute_reply":"2024-02-27T17:19:49.148409Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pdb\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nimport numpy as np\n\ndef MySoftmax(x, dim=-1, mask=None):\n    maxes = torch.max(x*mask, dim, keepdim=True)[0]\n    x_exp = torch.exp(x-maxes)\n\n    if mask is not None:\n        x_exp_sum = torch.sum(x_exp*mask, dim, keepdim=True)\n        return x_exp * mask / (x_exp_sum + 1e-10)\n    else:\n        x_exp_sum = torch.sum(x_exp, dim, keepdim=True)\n        return x_exp / x_exp_sum\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass PreNorm_branch(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        # self.fn = fn\n    def forward(self, x):\n        return self.norm(x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention_branch(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x, branch_num):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        qs = qkv[0]\n        ks = qkv[1]\n        vs = qkv[2]\n        per_branch = int((qs.size()[1] - branch_num) // branch_num)\n        outputs = []\n        cls_outputs = []\n        for i in range(branch_num):\n            q = torch.cat((qs[:, i:(i+1), :], qs[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n            k = torch.cat((ks[:, i:(i+1), :], ks[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            k = rearrange(k, 'b n (h d) -> b h n d', h = self.heads)\n            v = torch.cat((vs[:, i:(i+1), :], vs[:, (i*per_branch+branch_num) : ((i+1)*per_branch + branch_num), :] ), dim=1)\n            v = rearrange(v, 'b n (h d) -> b h n d', h = self.heads)\n\n            dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n            attn = self.attend(dots)\n            attn = self.dropout(attn)\n            out = torch.matmul(attn, v)\n            out = rearrange(out, 'b h n d -> b n (h d)')\n            cls_outputs.append(out[:,:1])\n            outputs.append(out[:,1:])\n        out = torch.cat(outputs, dim=1)\n        cls_outputs = torch.cat(cls_outputs, dim=1)\n        out = torch.cat((cls_outputs, out), dim=1)\n        # q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        # dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n        # attn = self.attend(dots)\n\n        # device = x.get_device()\n        # branch_masks = branch_masks.to(device)\n        # attn_branches = 0\n        # for i in range(branch_masks.size()[1]):\n        #     attn = MySoftmax(dots, dim = -1, mask = branch_masks[:,i])#self.attend(dots)\n        #     attn_branches += attn\n        # attn = self.dropout(attn)\n\n        # out = torch.matmul(attn, v)\n        # out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer_branch(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm_branch(dim),\n                Attention_branch(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                PreNorm_branch(dim),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n    def forward(self, x, branch_num):\n        for pn1, attn, pn2, ff in self.layers:\n            x1 = pn1(x)\n            x = attn(x1, branch_num) + x\n            x1 = pn2(x)\n            x = ff(x1) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., num_position = 16, branch_num = 2):\n        super().__init__()\n        self.mask_ratio = 0.5\n\n        self.audio_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wronng\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        self.rgb_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.face_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.speech_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.ocr_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n        \n        self.scene_embedding = nn.Sequential(\n            nn.LayerNorm(256), #might be wrong\n            nn.Linear(256, dim),\n            nn.LayerNorm(dim)\n        )\n\n        per_branch = num_position // branch_num\n        branch_masks = []\n        for i in range(branch_num):\n            branch_mask1 = torch.zeros((1, 1, heads, num_position + branch_num, num_position + branch_num))\n            branch_mask1[:, :, :, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num), (i*per_branch + branch_num) :((i+1)*per_branch + branch_num)] = 1\n            branch_mask1[:, :, :, i, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num)] = 1 # cls mask\n            branch_mask1[:, :, :, (i*per_branch + branch_num) :((i+1)*per_branch + branch_num), i] = 1 # cls mask\n            branch_mask1[:,:,:,i,i] = 1\n            branch_masks.append(branch_mask1)\n        self.branch_masks = torch.cat(branch_masks, dim=1) #[1, branch_num, heads, num_position, num_position]\n        self.branch_num = branch_num\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_position + branch_num, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, branch_num, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer_branch(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, rgb, audio, face, speech, ocr, scene, rgb_mask, audio_mask, face_mask, speech_mask, ocr_mask, scene_mask):\n        rgb = self.rgb_embedding(rgb)\n        audio = self.audio_embedding(audio)\n        face = self.face_embedding(face)\n        speech = self.speech_embedding(speech)\n        ocr = self.ocr_embedding(ocr)\n        scene = self.scene_embedding(scene)\n        \n        features = rgb * rgb_mask + audio * audio_mask + face * face_mask + speech * speech_mask + ocr * ocr_mask + scene* scene_mask\n        \n        b, n, _ = features.shape\n        x = features + self.pos_embedding[:, self.branch_num:]\n\n        cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)\n        cls_tokens = cls_tokens + self.pos_embedding[:, :self.branch_num]\n        x = torch.cat((cls_tokens, x), dim=1)\n        \n        x = self.transformer(x, self.branch_num)\n\n        x = x[:, :self.branch_num]\n\n        x = self.to_latent(x)\n        x = self.mlp_head(x)\n\n        return x\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:49.151087Z","iopub.execute_input":"2024-02-27T17:19:49.151441Z","iopub.status.idle":"2024-02-27T17:19:49.192279Z","shell.execute_reply.started":"2024-02-27T17:19:49.151414Z","shell.execute_reply":"2024-02-27T17:19:49.191300Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\nimport pdb\nimport torch\nimport argparse\nimport tqdm\nimport os\nimport numpy as np\nimport torch.nn as nn\nimport random\nimport warnings\nfrom torch.cuda.amp import GradScaler\nimport torch.nn.functional as F\nimport datetime\n\ndef dict_to_cuda(data):\n    for key, value in data.items():\n        data[key] = value.cuda()\n    return data\n\ndef spatialtemporal2tokens(data):\n    b, c, f, h, w = data.size()\n    data = data.view(b, c, f * h * w)\n    data = data.transpose(1, 2).contiguous()\n    return data\n\nclass LabelSmoothLoss(nn.Module):\n    def __init__(self, smoothing=0.0):\n        super(LabelSmoothLoss, self).__init__()\n        self.smoothing = smoothing\n\n    def forward(self, input, target):\n        log_prob = F.log_softmax(input, dim=-1)\n        weight = input.new_ones(input.size()) * self.smoothing / (input.size(-1) - 1.0)\n        weight.scatter_(-1, target.unsqueeze(-1), (1.0 - self.smoothing))\n        loss = (-weight * log_prob).sum(dim=-1).mean()\n        return loss\n\nclass AlignmentModule(nn.Module):\n    def __init__(self, dim=256):\n        super(AlignmentModule, self).__init__()\n        self.base_vectors = nn.Parameter(torch.randn(1, 128, 256)) #changed from 3086 to 128\n    \n    def forward(self, input):\n        # input [batchsize, n, 256]\n        input = torch.mean(input, dim=1, keepdim=True) # [batch_size, 1, 256]\n        #print(\"input\",input.shape)\n        base_vectors = self.base_vectors.repeat(input.size()[0], 1, 1)\n        #print(\"base_vectors\",base_vectors.shape)\n        sim = torch.mean((base_vectors - input) ** 2, dim=-1)\n        return sim\n\ndef extract_features(unimodal_models, data):\n    outputs = {}\n    for key, value in data.items():\n        outputs[key] = unimodal_models[key](value)\n        if key == 'RGB':\n            outputs[key] = spatialtemporal2tokens(outputs[key])\n    return outputs","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:49.193543Z","iopub.execute_input":"2024-02-27T17:19:49.193857Z","iopub.status.idle":"2024-02-27T17:19:49.208674Z","shell.execute_reply.started":"2024-02-27T17:19:49.193831Z","shell.execute_reply":"2024-02-27T17:19:49.207762Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class MaxMarginRankingLoss(nn.Module):\n    def __init__(self, margin=1):\n        super(MaxMarginRankingLoss, self).__init__()\n        self.loss = torch.nn.MarginRankingLoss(margin)\n        self.margin = margin\n\n    def forward(self,x):\n        n = x.size()[0]\n        \n        x1 = torch.diag(x)\n        x1 = x1.unsqueeze(1)\n        x1 = x1.expand(n, n)\n        x1 = x1.contiguous().view(-1,1)\n        x1 = torch.cat((x1,x1),0) \n\n        x2 = x.view(-1,1)\n        x3 = x.transpose(0,1).contiguous().view(-1,1)\n       \n        x2 = torch.cat((x2,x3),0)\n         \n        max_margin = F.relu(self.margin - (x1 - x2))\n        return max_margin.mean()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:49.209931Z","iopub.execute_input":"2024-02-27T17:19:49.210651Z","iopub.status.idle":"2024-02-27T17:19:49.222311Z","shell.execute_reply.started":"2024-02-27T17:19:49.210616Z","shell.execute_reply":"2024-02-27T17:19:49.221564Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.deterministic = True     #runs the same every time\ntorch.backends.cudnn.benchmark = False \nnp.random.seed(0)\ntorch.manual_seed(0)\nrandom.seed(0)\nwarnings.filterwarnings(\"ignore\")\ndevice = \"cuda\"  # or 'cpu'\ndevice = torch.device(device)\nbatch_size = 128\ntarget_length = 128\nmultimodal_model = ViT(\n        num_classes=text_dim,  #learnable tokens for the feature alignment loss(MSR_VTT= 128)\n        dim=256, #d* -> which is the dim of the common space tokens\n        depth=6, #number of transformer layers\n        heads=8, \n        mlp_dim=512, #hidden dimension of feed-forward layer\n        num_position=16 #k* -> number of tokens in common space\n    )\nmultimodal_model = torch.nn.DataParallel(multimodal_model)\nmultimodal_model = multimodal_model.to(device)\n\n    #feature projection model initialization\nreorganization_module = ViTReorganization(\n        dim=256,\n        depth=6,\n        heads=8,\n        mlp_dim=512, \n)\nreorganization_module = torch.nn.DataParallel(reorganization_module)\nreorganization_module = reorganization_module.to(device)\n\nalignment_model = AlignmentModule()\nalignment_model = torch.nn.DataParallel(alignment_model)\nalignment_model = alignment_model.to(device)\n\nloss_fn = LabelSmoothLoss(smoothing=0.1)\nloss_fn = loss_fn.cuda()\n\noptim = torch.optim.SGD(\n    list(multimodal_model.parameters())+list(reorganization_module.parameters()) + list(alignment_model.parameters()), lr=0.01, momentum=0.9, weight_decay=5e-4\n)\n### maybe change learning rate to lr=0.01\n\ntrain_dataset = MSRVTTTrain(\n        split=\"train\",\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\nvalidation_dataset = MSRVTTValidation(\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=train_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n    )\n\nval_loader = torch.utils.data.DataLoader(\n        validation_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=validation_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n    )\n\nloss_fn = MaxMarginRankingLoss(margin=0.09381161988446174)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:10:09.215306Z","iopub.execute_input":"2024-02-27T18:10:09.216178Z","iopub.status.idle":"2024-02-27T18:10:09.674458Z","shell.execute_reply.started":"2024-02-27T18:10:09.216143Z","shell.execute_reply":"2024-02-27T18:10:09.673421Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def verbose(epoch, status, metrics, name='TEST'):\n    print(name + ' - epoch: %d, epoch status: %.2f, r@1: %.3f, r@5: %.3f, r@10: %.3f, mr: %d' %\n          (epoch + 1, status,\n           metrics['R1'], metrics['R5'], metrics['R10'],\n           metrics['MR']))\n\n\ndef compute_metric(x):\n    sx = np.sort(-x, axis=1)\n    d = np.diag(-x)\n    d = d[:, np.newaxis]\n    ind = sx - d\n    ind = np.where(ind == 0)\n    ind = ind[1]\n\n    metrics = {}\n    metrics['R1'] = float(np.sum(ind == 0)) / len(ind)\n    metrics['R5'] = float(np.sum(ind < 5)) / len(ind)\n    metrics['R10'] = float(np.sum(ind < 10)) / len(ind)\n    metrics['MR'] = np.median(ind) + 1\n\n    return metrics\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:50.012938Z","iopub.execute_input":"2024-02-27T17:19:50.013322Z","iopub.status.idle":"2024-02-27T17:19:50.022146Z","shell.execute_reply.started":"2024-02-27T17:19:50.013286Z","shell.execute_reply":"2024-02-27T17:19:50.021235Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def verbose(epoch, mode, metrics, name=\"TEST\"):\n    r1, r5, r10, r50 = metrics[\"R1\"], metrics[\"R5\"], metrics[\"R10\"], metrics[\"R50\"]\n    msg = f\"{name:s} : {epoch}, R@1: {r1:.1f}\"\n    msg += f\", R@5: {r5:.1f}, R@10 {r10:.1f}, R@50 {r50:.1f}\"\n    msg += f\" MedR: {metrics['MedR']:g}, MeanR: {metrics['MeanR']:.1f}\"\n    print(msg)\n    \ndef v2t_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing captions from the dataset\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n\n    NOTES: We find the closest \"GT caption\" in the style of VSE, which corresponds\n    to finding the rank of the closest relevant caption in embedding space:\n    github.com/ryankiros/visual-semantic-embedding/blob/master/evaluation.py#L52-L56\n    \"\"\"\n    # switch axes of text and video\n    sims = sims.T\n\n    if False:\n        # experiment with toy example\n        sims = np.ones((3, 3))\n        sims[0, 0] = 2\n        sims[1, 1:2] = 2\n        sims[2, :] = 2\n        query_masks = None\n\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_caps = sims.shape\n    dists = -sims\n    caps_per_video = num_caps // num_queries\n    break_ties = \"averaging\"\n\n    MISSING_VAL = 1E8\n    query_ranks = []\n    for ii in range(num_queries):\n        row_dists = dists[ii, :]\n        if query_masks is not None:\n            # Set missing queries to have a distance of infinity.  A missing query\n            # refers to a query position `n` for a video that had less than `n`\n            # captions (for example, a few MSRVTT videos only have 19 queries)\n            row_dists[np.logical_not(query_masks.reshape(-1))] = MISSING_VAL\n\n        # NOTE: Using distance subtraction to perform the ranking is easier to make\n        # deterministic than using argsort, which suffers from the issue of defining\n        # \"stability\" for equal distances.  Example of distance subtraction code:\n        # github.com/antoine77340/Mixture-of-Embedding-Experts/blob/master/train.py\n        sorted_dists = np.sort(row_dists)\n\n        min_rank = np.inf\n        for jj in range(ii * caps_per_video, (ii + 1) * caps_per_video):\n            if row_dists[jj] == MISSING_VAL:\n                # skip rankings of missing captions\n                continue\n            ranks = np.where((sorted_dists - row_dists[jj]) == 0)[0]\n            if break_ties == \"optimistically\":\n                rank = ranks[0]\n            elif break_ties == \"averaging\":\n                # NOTE: If there is more than one caption per video, its possible for the\n                # method to do \"worse than chance\" in the degenerate case when all\n                # similarities are tied.  TODO(Samuel): Address this case.\n                rank = ranks.mean()\n            if rank < min_rank:\n                min_rank = rank\n        query_ranks.append(min_rank)\n    query_ranks = np.array(query_ranks)\n\n    # sanity check against old version of code\n    if False:\n        sorted_dists = np.sort(dists, axis=1)\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        rows_old, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        if rows_old.size > num_queries:\n            _, idx = np.unique(rows_old, return_index=True)\n            cols_old = cols_old[idx]\n        num_diffs = (1 - (cols_old == query_ranks)).sum()\n        msg = f\"new metric doesn't match in {num_diffs} places\"\n        assert np.array_equal(cols_old, query_ranks), msg\n\n        # visualise the distance matrix\n        import sys\n        import matplotlib\n        matplotlib.use(\"Agg\")\n        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n\n    return cols2metrics(query_ranks, num_queries)\n\ndef cols2metrics(cols, num_queries):\n    metrics = {}\n    metrics[\"R1\"] = 100 * float(np.sum(cols == 0)) / num_queries\n    metrics[\"R5\"] = 100 * float(np.sum(cols < 5)) / num_queries\n    metrics[\"R10\"] = 100 * float(np.sum(cols < 10)) / num_queries\n    metrics[\"R50\"] = 100 * float(np.sum(cols < 50)) / num_queries\n    metrics[\"MedR\"] = np.median(cols) + 1\n    metrics[\"MeanR\"] = np.mean(cols) + 1\n    stats = [metrics[x] for x in (\"R1\", \"R5\", \"R10\")]\n    return metrics\n\ndef t2v_metrics(sims, query_masks=None):\n    \"\"\"Compute retrieval metrics from a similiarity matrix.\n\n    Args:\n        sims (th.Tensor): N x M matrix of similarities between embeddings, where\n             x_{i,j} = <text_embd[i], vid_embed[j]>\n        query_masks (th.Tensor): mask any missing queries from the dataset (two videos\n             in MSRVTT only have 19, rather than 20 captions)\n\n    Returns:\n        (dict[str:float]): retrieval metrics\n    \"\"\"\n    assert sims.ndim == 2, \"expected a matrix\"\n    num_queries, num_vids = sims.shape\n    dists = -sims\n    sorted_dists = np.sort(dists, axis=1)\n\n    if False:\n        import sys\n        import matplotlib\n        from pathlib import Path\n        matplotlib.use(\"Agg\")\n        import matplotlib.pyplot as plt\n        sys.path.insert(0, str(Path.home() / \"coding/src/zsvision/python\"))\n        from zsvision.zs_iterm import zs_dispFig # NOQA\n        plt.matshow(dists)\n        zs_dispFig()\n        import ipdb; ipdb.set_trace()\n\n    # The indices are computed such that they slice out the ground truth distances\n    # from the psuedo-rectangular dist matrix\n    queries_per_video = num_queries // num_vids\n    gt_idx = [[np.ravel_multi_index([ii, jj], (num_queries, num_vids))\n              for ii in range(jj * queries_per_video, (jj + 1) * queries_per_video)]\n              for jj in range(num_vids)]\n    gt_idx = np.array(gt_idx)\n    gt_dists = dists.reshape(-1)[gt_idx.reshape(-1)]\n    gt_dists = gt_dists[:, np.newaxis]\n    rows, cols = np.where((sorted_dists - gt_dists) == 0)  # find column position of GT\n\n    # --------------------------------\n    # NOTE: Breaking ties\n    # --------------------------------\n    # We sometimes need to break ties (in general, these should occur extremely rarely,\n    # but there are pathological cases when they can distort the scores, such as when\n    # the similarity matrix is all zeros). Previous implementations (e.g. the t2i\n    # evaluation function used\n    # here: https://github.com/niluthpol/multimodal_vtt/blob/master/evaluation.py and\n    # here: https://github.com/linxd5/VSE_Pytorch/blob/master/evaluation.py#L87) generally\n    # break ties \"optimistically\".  However, if the similarity matrix is constant this\n    # can evaluate to a perfect ranking. A principled option is to average over all\n    # possible partial orderings implied by the ties. See # this paper for a discussion:\n    #    McSherry, Frank, and Marc Najork,\n    #    \"Computing information retrieval performance measures efficiently in the presence\n    #    of tied scores.\" European conference on information retrieval. Springer, Berlin, \n    #    Heidelberg, 2008.\n    # http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8892&rep=rep1&type=pdf\n\n    # break_ties = \"optimistically\"\n    break_ties = \"averaging\"\n\n    if rows.size > num_queries:\n        assert np.unique(rows).size == num_queries, \"issue in metric evaluation\"\n        if break_ties == \"optimistically\":\n            _, idx = np.unique(rows, return_index=True)\n            cols = cols[idx]\n        elif break_ties == \"averaging\":\n            # fast implementation, based on this code:\n            # https://stackoverflow.com/a/49239335\n            locs = np.argwhere((sorted_dists - gt_dists) == 0)\n\n            # Find the split indices\n            steps = np.diff(locs[:, 0])\n            splits = np.nonzero(steps)[0] + 1\n            splits = np.insert(splits, 0, 0)\n\n            # Compute the result columns\n            summed_cols = np.add.reduceat(locs[:, 1], splits)\n            counts = np.diff(np.append(splits, locs.shape[0]))\n            avg_cols = summed_cols / counts\n            if False:\n                print(\"Running slower code to verify rank averaging across ties\")\n                # slow, but more interpretable version, used for testing\n                avg_cols_slow = [np.mean(cols[rows == idx]) for idx in range(num_queries)]\n                assert np.array_equal(avg_cols, avg_cols_slow), \"slow vs fast difference\"\n                print(\"passed num check\")\n            cols = avg_cols\n\n    msg = \"expected ranks to match queries ({} vs {}) \"\n    if cols.size != num_queries:\n        import ipdb; ipdb.set_trace()\n    assert cols.size == num_queries, msg\n\n    if False:\n        # overload mask to check that we can recover the scores for single-query\n        # retrieval\n        print(\"DEBUGGING MODE\")\n        query_masks = np.zeros_like(query_masks)\n        query_masks[:, 0] = 1  # recover single query score\n\n    if query_masks is not None:\n        # remove invalid queries\n        assert query_masks.size == num_queries, \"invalid query mask shape\"\n        cols = cols[query_masks.reshape(-1).astype(np.bool)]\n        assert cols.size == query_masks.sum(), \"masking was not applied correctly\"\n        # update number of queries to account for those that were missing\n        num_queries = query_masks.sum()\n\n    if False:\n        # sanity check against old logic for square matrices\n        gt_dists_old = np.diag(dists)\n        gt_dists_old = gt_dists_old[:, np.newaxis]\n        _, cols_old = np.where((sorted_dists - gt_dists_old) == 0)\n        assert np.array_equal(cols_old, cols), \"new metric doesn't match\"\n\n    return cols2metrics(cols, num_queries)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-02-27T17:19:50.023734Z","iopub.execute_input":"2024-02-27T17:19:50.024092Z","iopub.status.idle":"2024-02-27T17:19:50.059756Z","shell.execute_reply.started":"2024-02-27T17:19:50.024058Z","shell.execute_reply":"2024-02-27T17:19:50.058863Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import time\nepoch_outputs = []\nBestLoss = float(\"inf\")\nBestEpoch = 0\nprint(\"---------------Start Training---------------\")\nfor epoch_i in range(50):\n    all_truth_outputs = [] \n    all_keys =[]\n    start_time = time.time()\n    dataloaders = {\"train\": train_loader, \"val\":val_loader}\n    split = \"train\"\n    scaler = GradScaler()\n    count = 0\n    count_val = 0\n    total_loss = 0\n    total_loss_val = 0\n    loss = 0\n    multimodal_model.train(split == \"train\")\n    reorganization_module.train(split == \"train\")\n    for (\n        i,\n        (\n            data,\n            labels,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n            key\n        ),\n    ) in enumerate(dataloaders[\"train\"]):\n        \n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n        outputs = data \n        \n        rgb, audio, face, speech, ocr, scene= reorganization_module(\n        outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"]\n        )\n        \n        audio_sim = alignment_model(audio)\n        rgb_sim = alignment_model(rgb)\n        face_sim = alignment_model(face)\n        speech_sim = alignment_model(speech)\n        ocr_sim = alignment_model(ocr)\n        scene_sim = alignment_model(scene)\n        \n        ###first column is for ground truth labels and secod column for pseudo labels\n        outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene,  masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene']\n        )\n        ###calculate indices\n        audio_indices = torch.sum(masks['Audio'].squeeze(-1), dim=-1) > 0\n        rgb_indices = torch.sum(masks['RGB'].squeeze(-1), dim=-1) > 0\n        face_indices = torch.sum(masks['Face'].squeeze(-1), dim=-1) > 0\n        speech_indices = torch.sum(masks['Speech'].squeeze(-1), dim=-1) > 0\n        ocr_indices = torch.sum(masks['OCR'].squeeze(-1), dim=-1) > 0\n        scene_indices = torch.sum(masks['Scene'].squeeze(-1), dim=-1) > 0\n\n        ###calculate ground truth loss\n        truth_outputs = outputs[:,0,:]\n        detached_outputs = truth_outputs.detach().cpu()\n        all_truth_outputs.append(detached_outputs)\n        all_keys.extend(key)\n        \n        truth_labels = labels.mean(dim=1)\n        truth_outputs = F.normalize(truth_outputs, p=2, dim=1)\n        truth_labels = F.normalize(truth_labels, p=2, dim=1)\n        truth_outputs = truth_outputs.to(device)\n        truth_labels = truth_labels.to(device)\n        similarity_matrix = torch.matmul(truth_outputs, truth_labels.t())\n        loss_supervised = loss_fn(similarity_matrix)\n        \n        ###calculate pseudo loss\n        pseudo_outputs = outputs[:,1,:]\n        pseudo_outputs = F.normalize(pseudo_outputs, p=2, dim=1)\n        pseudo_labels = F.normalize(rgb_pseudo, p=2, dim=1)\n        pseudo_outputs = pseudo_outputs.to(device)\n        pseudo_labels = pseudo_labels.to(device)\n        similarity_matrix = torch.matmul(pseudo_outputs, pseudo_labels.t())\n        loss_pseudo = loss_fn(similarity_matrix)\n        \n        #check if modalities exist\n        audio_sim = audio_sim[audio_indices]\n        rgb_sim = rgb_sim[rgb_indices]\n        face_sim = face_sim[face_indices]\n        speech_sim = speech_sim[speech_indices]\n        ocr_sim = ocr_sim[ocr_indices]\n        scene_sim = scene_sim[scene_indices]\n        \n        ###calculate alignment loss\n        audio_sim, _ = torch.min(audio_sim, dim=-1)\n        rgb_sim, _ = torch.min(rgb_sim, dim=-1)\n        face_sim, _ = torch.min(face_sim, dim=-1)\n        speech_sim, _ = torch.min(speech_sim, dim=-1)\n        ocr_sim, _ = torch.min(ocr_sim, dim=-1)\n        scene_sim, _ = torch.min(scene_sim, dim=-1)\n        \n        alignment_loss = (torch.sum(rgb_sim)+torch.sum(audio_sim)+torch.sum(face_sim)+torch.sum(speech_sim)+torch.sum(ocr_sim)+torch.sum(scene_sim))/torch.sum(rgb_indices)+(torch.sum(audio_indices)+ torch.sum(face_indices)+ torch.sum(speech_indices)+ torch.sum(ocr_indices)+ torch.sum(scene_indices))\n\n        confusion_matrix = similarity_matrix.data.cpu().float().numpy()\n        metrics = compute_metric(confusion_matrix)\n        #verbose(epoch_i,batch_size * float(i) / len(dataset),metrics, 'TRAIN')\n        ###calculate total loss\n        \n        loss = alignment_loss*0.001 + loss_supervised\n        #loss = alignment_loss*0.001 + loss_supervised + loss_pseudo\n        #loss = loss_supervised + loss_pseudo\n        #loss = loss_supervised\n        \n        optim.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optim)\n        scaler.update()\n\n        total_loss += loss.item() * batch_size\n        #count+= batch_size\n        count += outputs.size()[0]\n    \n    final_truth_outputs = torch.cat(all_truth_outputs, dim=0)\n    final_truth_outputs_np = final_truth_outputs.numpy()\n    if ((epoch_i+1)>49):   \n        #epoch_outputs.append(final_truth_outputs_np)\n        np.save(\"/kaggle/working/pseudo_labels.npy\", final_truth_outputs_np)\n        \n        with open(\"/kaggle/working/keys.txt\", \"w\") as f:\n            for id in all_keys:\n                f.write(f\"{id}\\n\")\n    all_truth_outputs.clear()\n    all_keys.clear()\n       \n    for (\n        i,\n        (\n            data,\n            labels,\n            masks,\n            audio_pseudo,\n            rgb_pseudo,\n            face_pseudo,\n            speech_pseudo,\n            ocr_pseudo,\n            scene_pseudo,\n        ),\n    ) in enumerate(dataloaders[\"val\"]):\n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n         \n        \n        with torch.no_grad():\n            outputs = data\n            rgb, audio, face, speech, ocr, scene= reorganization_module(\n            outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"]\n            )\n            outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene,  masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene']\n            )\n            ###calculate ground truth loss\n            truth_outputs = outputs[:,0,:]\n            truth_labels = labels.mean(dim=1)\n            truth_outputs = F.normalize(truth_outputs, p=2, dim=1)\n            truth_labels = F.normalize(truth_labels, p=2, dim=1)\n            truth_outputs = truth_outputs.to(device)\n            truth_labels = truth_labels.to(device)\n            similarity_matrix = torch.matmul(truth_outputs, truth_labels.t())\n            loss_supervised = loss_fn(similarity_matrix)\n            confusion_matrix = similarity_matrix.data.cpu().float().numpy()\n            #metrics = compute_metric(confusion_matrix)\n            metrics = v2t_metrics(confusion_matrix)\n            verbose(\"Video to Text\",batch_size * float(i) / len(validation_dataset),metrics, 'Validation')\n            total_loss_val += loss_supervised.item() * batch_size\n            #count+= batch_size\n            count_val += outputs.size()[0]\n            \n    if (total_loss_val / float(count_val)) < BestLoss:\n            BestLoss = total_loss_val / float(count_val)\n            BestEpoch = epoch_i+1\n            save = {\n                    \"epoch\": epoch_i+1,\n                    \"model\": multimodal_model.state_dict(),\n                    \"reorganization\": reorganization_module.state_dict(),\n                    \"alignment\": alignment_model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"best_loss\": BestLoss,\n            }\n\n            torch.save(\n                  save,  \"/kaggle/working/best_multimodal_openai_BestLoss_align.pt\"\n            )\n    if (epoch_i>48):\n            BestLoss = total_loss_val / float(count_val)\n            BestEpoch = epoch_i+1\n            save = {\n                    \"epoch\": epoch_i+1,\n                    \"model\": multimodal_model.state_dict(),\n                    \"reorganization\": reorganization_module.state_dict(),\n                    \"alignment\": alignment_model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"best_loss\": BestLoss,\n            }\n\n            torch.save(\n                  save,  \"/kaggle/working/best_multimodal_openai_epoch50_align.pt\"\n            )\n             \n    print(\"Epoch: %02d\" % (epoch_i+1))\n    #print(\"Current Training loss: \", loss.item())\n    print(\"Average Training loss: \", total_loss/float(count))\n    print(\"Average Validation loss: \", total_loss_val/float(count_val))\n    end_time = time.time()\n    dur = end_time-start_time\n    print(f\"Epoch Duration: {dur:.2f} seconds\")\n    print(\"---------------------------------------\")\n#stacked_outputs = np.stack(epoch_outputs)\n#average_output = np.mean(stacked_outputs, axis=0)\n#np.save(\"/kaggle/working/pseudo_labels.npy\", average_output)","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:10:11.462988Z","iopub.execute_input":"2024-02-27T18:10:11.463535Z","iopub.status.idle":"2024-02-27T18:19:25.651056Z","shell.execute_reply.started":"2024-02-27T18:10:11.463498Z","shell.execute_reply":"2024-02-27T18:19:25.649881Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"---------------Start Training---------------\nValidation : Video to Text, R@1: 1.6, R@5: 7.9, R@10 11.0, R@50 48.0 MedR: 55, MeanR: 55.8\nEpoch: 01\nAverage Training loss:  0.3083491911815642\nAverage Validation loss:  0.09260643185593012\nEpoch Duration: 10.98 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 2.4, R@5: 7.9, R@10 14.2, R@50 56.7 MedR: 42, MeanR: 48.3\nEpoch: 02\nAverage Training loss:  0.30365701221786445\nAverage Validation loss:  0.08876629326287216\nEpoch Duration: 11.66 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 3.1, R@5: 7.9, R@10 20.5, R@50 65.4 MedR: 33, MeanR: 40.6\nEpoch: 03\nAverage Training loss:  0.2946825535564988\nAverage Validation loss:  0.08088551919291338\nEpoch Duration: 11.25 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 4.7, R@5: 12.6, R@10 25.2, R@50 73.2 MedR: 25, MeanR: 34.6\nEpoch: 04\nAverage Training loss:  0.281062994906746\nAverage Validation loss:  0.07106726188359298\nEpoch Duration: 11.47 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 8.7, R@5: 15.7, R@10 30.7, R@50 78.0 MedR: 19, MeanR: 30.5\nEpoch: 05\nAverage Training loss:  0.2706535338769764\nAverage Validation loss:  0.0638615916094442\nEpoch Duration: 11.19 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 8.7, R@5: 20.5, R@10 36.2, R@50 80.3 MedR: 19, MeanR: 28.4\nEpoch: 06\nAverage Training loss:  0.263342120104555\nAverage Validation loss:  0.05740352315226878\nEpoch Duration: 11.36 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 7.9, R@5: 24.4, R@10 38.6, R@50 81.9 MedR: 16, MeanR: 26.6\nEpoch: 07\nAverage Training loss:  0.2590429539433753\nAverage Validation loss:  0.05398170966801681\nEpoch Duration: 11.56 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 7.9, R@5: 25.2, R@10 40.2, R@50 83.5 MedR: 16, MeanR: 25.1\nEpoch: 08\nAverage Training loss:  0.2559379988801693\nAverage Validation loss:  0.05073384788092666\nEpoch Duration: 11.12 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 27.6, R@10 41.7, R@50 85.0 MedR: 15, MeanR: 24.1\nEpoch: 09\nAverage Training loss:  0.2537793795414167\nAverage Validation loss:  0.04854365972083385\nEpoch Duration: 11.23 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 26.8, R@10 42.5, R@50 85.8 MedR: 14, MeanR: 23.5\nEpoch: 10\nAverage Training loss:  0.2525204881515515\nAverage Validation loss:  0.04718499671755813\nEpoch Duration: 11.55 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 28.3, R@10 42.5, R@50 85.0 MedR: 14, MeanR: 22.9\nEpoch: 11\nAverage Training loss:  0.2508952765901865\nAverage Validation loss:  0.04571492277731107\nEpoch Duration: 11.15 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 33.1, R@10 43.3, R@50 85.0 MedR: 12, MeanR: 22.4\nEpoch: 12\nAverage Training loss:  0.2497140640453243\nAverage Validation loss:  0.044939694442148286\nEpoch Duration: 11.30 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 31.5, R@10 44.9, R@50 88.2 MedR: 12, MeanR: 21.5\nEpoch: 13\nAverage Training loss:  0.24850602913576392\nAverage Validation loss:  0.043771935260202004\nEpoch Duration: 11.55 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 32.3, R@10 47.2, R@50 88.2 MedR: 12, MeanR: 21.1\nEpoch: 14\nAverage Training loss:  0.24793763953603834\nAverage Validation loss:  0.043218394902747446\nEpoch Duration: 11.60 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 32.3, R@10 49.6, R@50 88.2 MedR: 11, MeanR: 20.9\nEpoch: 15\nAverage Training loss:  0.24667157033026796\nAverage Validation loss:  0.04250714159387303\nEpoch Duration: 11.11 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 33.1, R@10 50.4, R@50 88.2 MedR: 10, MeanR: 20.6\nEpoch: 16\nAverage Training loss:  0.24625354733460514\nAverage Validation loss:  0.041822294550617846\nEpoch Duration: 11.25 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 32.3, R@10 51.2, R@50 89.0 MedR: 10, MeanR: 20.2\nEpoch: 17\nAverage Training loss:  0.24526337304826196\nAverage Validation loss:  0.04195798663642463\nEpoch Duration: 10.64 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 34.6, R@10 51.2, R@50 89.0 MedR: 10, MeanR: 20.0\nEpoch: 18\nAverage Training loss:  0.24450290995626686\nAverage Validation loss:  0.04145425886619748\nEpoch Duration: 11.53 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 35.4, R@10 51.2, R@50 89.0 MedR: 9, MeanR: 19.9\nEpoch: 19\nAverage Training loss:  0.24426705500835394\nAverage Validation loss:  0.040566928743377445\nEpoch Duration: 11.23 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.0, R@5: 34.6, R@10 49.6, R@50 89.0 MedR: 11, MeanR: 19.6\nEpoch: 20\nAverage Training loss:  0.24317486026622762\nAverage Validation loss:  0.04020544863122655\nEpoch Duration: 11.20 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 36.2, R@10 52.8, R@50 89.0 MedR: 10, MeanR: 19.5\nEpoch: 21\nAverage Training loss:  0.24355690749801104\nAverage Validation loss:  0.04032410599115327\nEpoch Duration: 10.76 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 37.0, R@10 52.0, R@50 89.0 MedR: 10, MeanR: 19.3\nEpoch: 22\nAverage Training loss:  0.2427840459077713\nAverage Validation loss:  0.040036730878935084\nEpoch Duration: 11.11 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 37.0, R@10 52.0, R@50 89.8 MedR: 9, MeanR: 19.1\nEpoch: 23\nAverage Training loss:  0.24244622087471315\nAverage Validation loss:  0.039568168910469596\nEpoch Duration: 11.21 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 37.0, R@10 54.3, R@50 89.8 MedR: 9, MeanR: 18.5\nEpoch: 24\nAverage Training loss:  0.24213826445853784\nAverage Validation loss:  0.03864026257372278\nEpoch Duration: 11.37 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 40.2, R@10 55.1, R@50 89.8 MedR: 9, MeanR: 18.6\nEpoch: 25\nAverage Training loss:  0.24125434849717256\nAverage Validation loss:  0.038607499730868605\nEpoch Duration: 11.39 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 37.8, R@10 53.5, R@50 89.8 MedR: 9, MeanR: 18.5\nEpoch: 26\nAverage Training loss:  0.24101598087441403\nAverage Validation loss:  0.03893186539176881\nEpoch Duration: 10.62 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.0, R@5: 39.4, R@10 55.1, R@50 89.8 MedR: 9, MeanR: 18.7\nEpoch: 27\nAverage Training loss:  0.24086510089983795\nAverage Validation loss:  0.038079246761291985\nEpoch Duration: 11.39 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 40.2, R@10 55.1, R@50 89.8 MedR: 8, MeanR: 18.3\nEpoch: 28\nAverage Training loss:  0.24092966990776185\nAverage Validation loss:  0.03793827567513534\nEpoch Duration: 11.48 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.0, R@5: 38.6, R@10 55.9, R@50 89.8 MedR: 8, MeanR: 18.3\nEpoch: 29\nAverage Training loss:  0.2398877315910734\nAverage Validation loss:  0.038456218449149544\nEpoch Duration: 10.52 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 9.4, R@5: 41.7, R@10 55.9, R@50 90.6 MedR: 8, MeanR: 17.9\nEpoch: 30\nAverage Training loss:  0.24005511351377024\nAverage Validation loss:  0.03756080462237981\nEpoch Duration: 11.38 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 39.4, R@10 56.7, R@50 89.0 MedR: 8, MeanR: 17.8\nEpoch: 31\nAverage Training loss:  0.24023665924985965\nAverage Validation loss:  0.03775761822077233\nEpoch Duration: 10.50 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 39.4, R@10 56.7, R@50 89.8 MedR: 8, MeanR: 17.7\nEpoch: 32\nAverage Training loss:  0.23948285035341726\nAverage Validation loss:  0.03767255723007082\nEpoch Duration: 10.62 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 38.6, R@10 59.8, R@50 89.8 MedR: 8, MeanR: 17.6\nEpoch: 33\nAverage Training loss:  0.23948043373420677\nAverage Validation loss:  0.03756669562632643\nEpoch Duration: 10.85 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 12.6, R@5: 41.7, R@10 58.3, R@50 89.8 MedR: 9, MeanR: 17.5\nEpoch: 34\nAverage Training loss:  0.2390442225454627\nAverage Validation loss:  0.03681741549274114\nEpoch Duration: 11.11 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 13.4, R@5: 42.5, R@10 55.9, R@50 90.6 MedR: 8, MeanR: 17.3\nEpoch: 35\nAverage Training loss:  0.2387267176720215\nAverage Validation loss:  0.036808622164989084\nEpoch Duration: 11.23 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 13.4, R@5: 40.9, R@10 57.5, R@50 91.3 MedR: 7, MeanR: 17.1\nEpoch: 36\nAverage Training loss:  0.2382244573985855\nAverage Validation loss:  0.036460031674602836\nEpoch Duration: 11.45 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.8, R@5: 40.2, R@10 56.7, R@50 90.6 MedR: 8, MeanR: 17.2\nEpoch: 37\nAverage Training loss:  0.23838181410007134\nAverage Validation loss:  0.03671864261777382\nEpoch Duration: 10.55 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 10.2, R@5: 42.5, R@10 58.3, R@50 91.3 MedR: 8, MeanR: 17.2\nEpoch: 38\nAverage Training loss:  0.23797989740873945\nAverage Validation loss:  0.036549857282263086\nEpoch Duration: 10.65 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 12.6, R@5: 43.3, R@10 57.5, R@50 90.6 MedR: 8, MeanR: 16.8\nEpoch: 39\nAverage Training loss:  0.2381454908809372\nAverage Validation loss:  0.035990170606477995\nEpoch Duration: 11.21 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 14.2, R@5: 42.5, R@10 60.6, R@50 91.3 MedR: 7, MeanR: 17.0\nEpoch: 40\nAverage Training loss:  0.2377310310222029\nAverage Validation loss:  0.0359271642729992\nEpoch Duration: 11.37 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 12.6, R@5: 41.7, R@10 59.1, R@50 90.6 MedR: 8, MeanR: 16.9\nEpoch: 41\nAverage Training loss:  0.2374305145249615\nAverage Validation loss:  0.035952496716356655\nEpoch Duration: 10.62 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.8, R@5: 42.5, R@10 57.5, R@50 90.6 MedR: 7, MeanR: 17.0\nEpoch: 42\nAverage Training loss:  0.2371184578802881\nAverage Validation loss:  0.03607318157286156\nEpoch Duration: 10.43 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 15.0, R@5: 41.7, R@10 58.3, R@50 92.1 MedR: 8, MeanR: 16.8\nEpoch: 43\nAverage Training loss:  0.23714606186843185\nAverage Validation loss:  0.03622980568352647\nEpoch Duration: 10.64 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.8, R@5: 41.7, R@10 58.3, R@50 92.1 MedR: 7, MeanR: 16.9\nEpoch: 44\nAverage Training loss:  0.23706748718749027\nAverage Validation loss:  0.03546453460933655\nEpoch Duration: 11.54 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 15.0, R@5: 40.2, R@10 62.2, R@50 92.1 MedR: 8, MeanR: 16.7\nEpoch: 45\nAverage Training loss:  0.2366972149708808\nAverage Validation loss:  0.036241929362139366\nEpoch Duration: 10.52 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 11.8, R@5: 40.9, R@10 59.8, R@50 90.6 MedR: 8, MeanR: 16.7\nEpoch: 46\nAverage Training loss:  0.23696598215218825\nAverage Validation loss:  0.03578778890174205\nEpoch Duration: 10.46 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 14.2, R@5: 40.9, R@10 59.8, R@50 91.3 MedR: 8, MeanR: 16.6\nEpoch: 47\nAverage Training loss:  0.23661283038827693\nAverage Validation loss:  0.0358209384707954\nEpoch Duration: 10.71 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 12.6, R@5: 41.7, R@10 59.8, R@50 91.3 MedR: 8, MeanR: 16.5\nEpoch: 48\nAverage Training loss:  0.23606148217106201\nAverage Validation loss:  0.03570516090693436\nEpoch Duration: 10.36 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 13.4, R@5: 42.5, R@10 60.6, R@50 91.3 MedR: 8, MeanR: 16.5\nEpoch: 49\nAverage Training loss:  0.2360859254016756\nAverage Validation loss:  0.0355050244669276\nEpoch Duration: 10.38 seconds\n---------------------------------------\nValidation : Video to Text, R@1: 13.4, R@5: 44.9, R@10 61.4, R@50 90.6 MedR: 8, MeanR: 16.1\nEpoch: 50\nAverage Training loss:  0.2361014673791502\nAverage Validation loss:  0.03528064442431833\nEpoch Duration: 11.77 seconds\n---------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"## Testing###\nmultimodal_model = ViT(\n        num_classes=text_dim,  #learnable tokens for the feature alignment loss(MSR_VTT= 128)\n        dim=256, #d* -> which is the dim of the common space tokens\n        depth=6, #number of transformer layers\n        heads=8, \n        mlp_dim=512, #hidden dimension of feed-forward layer\n        num_position=16 #k* -> number of tokens in common space\n    )\nmultimodal_model = torch.nn.DataParallel(multimodal_model)\nmultimodal_model = multimodal_model.to(device)\n\n    #feature projection model initialization\nreorganization_module = ViTReorganization(\n        dim=256,\n        depth=6,\n        heads=8,\n        mlp_dim=512, \n)\nreorganization_module = torch.nn.DataParallel(reorganization_module)\nreorganization_module = reorganization_module.to(device)\n\ncheckpoint = torch.load(\"/kaggle/working/best_multimodal_openai_epoch50_align.pt\")\nmultimodal_model.load_state_dict(checkpoint['model'])\nmultimodal_model.eval()\n\nreorganization_module.load_state_dict(checkpoint['reorganization'])\nreorganization_module.eval()\n\ntest_dataset = MSRVTTTEST(\n        num_position=16,   # change to 16 for MSR-VTT\n        #label_data_path='/kaggle/input/msr-vtt-features-expert/structured-symlinks/aggregated_text_feats/w2v_MSRVTT.pickle'\n    )\n\ntest_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=test_dataset.collate_data,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=False,\n    )\n\nprint(\"---------------Start Testing ---------------\")\nprint(\"Checkpoint from: \" + str(checkpoint['epoch']) + \" epoch  with best loss \" + str(checkpoint['best_loss'])  )\n\naggregated_outputs = []\naggregated_labels = []\n\nfor (\n      i,\n      (\n        data,\n        labels,\n        masks,\n        audio_pseudo,\n        rgb_pseudo,\n        face_pseudo,\n        speech_pseudo,\n        ocr_pseudo,\n        scene_pseudo,\n     ),\n    ) in enumerate(test_loader):\n        data = dict_to_cuda(data)\n        masks = dict_to_cuda(masks)\n        labels = labels\n        audio_pseudo = audio_pseudo.cuda()\n        rgb_pseudo = rgb_pseudo.cuda()\n        face_pseudo = face_pseudo.cuda()\n        speech_pseudo = speech_pseudo.cuda()\n        ocr_pseudo = ocr_pseudo.cuda()\n        scene_pseudo = scene_pseudo.cuda()\n        \n        with torch.no_grad():\n            outputs = data\n            rgb, audio, face, speech, ocr, scene= reorganization_module(\n            outputs['RGB'], outputs['Audio'], outputs[\"Face\"], outputs[\"Speech\"], outputs[\"OCR\"], outputs[\"Scene\"]\n            )\n            outputs = multimodal_model(\n            rgb, audio, face, speech, ocr, scene,  masks['RGB'], masks['Audio'], masks['Face'], masks['Speech'], masks['OCR'], masks['Scene']\n            )\n            #prepare and add to list \n            truth_outputs = F.normalize(outputs[:, 0, :], p=2, dim=1).cpu()\n            truth_labels = F.normalize(labels.mean(dim=1), p=2, dim=1).cpu()\n            aggregated_outputs.append(truth_outputs)\n            aggregated_labels.append(truth_labels)\n            \n# Concatenate all collected outputs and labels\nfinal_outputs = torch.cat(aggregated_outputs, dim=0)\nfinal_labels = torch.cat(aggregated_labels, dim=0)\nsimilarity_matrix = torch.matmul(final_outputs, final_labels.t())\nconfusion_matrix = similarity_matrix.numpy()\nmetrics = v2t_metrics(confusion_matrix)\nverbose(\"Video to Text\",batch_size * float(i) / len(validation_dataset),metrics, 'Test')\nmetrics = t2v_metrics(confusion_matrix)\nverbose(\"Text to Video\",batch_size * float(i) / len(validation_dataset),metrics, 'Test')","metadata":{"execution":{"iopub.status.busy":"2024-02-27T18:24:59.657033Z","iopub.execute_input":"2024-02-27T18:24:59.657503Z","iopub.status.idle":"2024-02-27T18:25:01.725454Z","shell.execute_reply.started":"2024-02-27T18:24:59.657463Z","shell.execute_reply":"2024-02-27T18:25:01.724109Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"---------------Start Testing ---------------\nCheckpoint from: 50 epoch  with best loss 0.03528064442431833\nTest : Video to Text, R@1: 5.5, R@5: 17.9, R@10 28.4, R@50 64.4 MedR: 30, MeanR: 71.2\nTest : Text to Video, R@1: 3.7, R@5: 11.9, R@10 20.7, R@50 55.3 MedR: 43, MeanR: 91.7\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Model with Lsupervised**\n\nCheckpoint from: 20 epoch  with best loss 0.8163572897122601\n\nTest : Video to Text, R@1: **3.0**, R@5: **11.9**, R@10 **20.4** MedR: **51**, MeanR: **111.6**\n\nTest : Text to Video, R@1: **2.7**, R@5: **10.6**, R@10 **19.0** MedR: **44**, MeanR: **97.1**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lalign**\n\nCheckpoint from: 40 epoch  with best loss 0.8537058191975271\n\nTest : Video to Text, R@1: 3.4, R@5: 11.6, R@10 18.0, R@50 49.3 MedR: 52, MeanR: 122.7\n****\nTest : Text to Video, R@1: 2.5, R@5: 8.8, R@10 15.6, R@50 42.6 MedR: 69, MeanR: 141.0","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lsudo**\n\nCheckpoint from: 20 epoch  with best loss 0.8174096054918184\n\nTest : Video to Text, R@1: **2.2**, R@5: **12.0**, R@10 **19.9**, R@50 50.5 MedR: **50**, MeanR: **111.4**\n\nTest : Text to Video, R@1: **3.4**, R@5: **11.0**, R@10 **20.5**, R@50 54.2 MedR: **44**, MeanR: **96.7**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised and random labels**\n\nCheckpoint from: 20 epoch  with best loss 0.8158261156457616\n\nTest : Video to Text, R@1: 3.9, R@5: 12.7, R@10 20.4, R@50 54.6 MedR: 41, MeanR: **99.3**\n\nTest : Text to Video, R@1: 2.6, R@5: 9.9, R@10 18.7, R@50 52.8 MedR: 46, MeanR: **99.6**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised, OpenAI random labels**\n\nCheckpoint from: 50 epoch  with best loss 0.8630691288024421\n\nTest : Video to Text, R@1: 4.1, R@5: 13.7, R@10 21.8, R@50 52.5 MedR: 46, MeanR: **105.7**\n\nTest : Text to Video, R@1: 3.4, R@5: 10.3, R@10 17.8, R@50 48.4 MedR: 53, MeanR: **112.0**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.03814773109015517\n\nTest : Video to Text, R@1: **5.5**, R@5: 18.3, R@10 28.1, R@50 64.1 MedR: **30**, MeanR: **71.3**\n\nTest : Text to Video, R@1: **3.7**, R@5: 11.9, R@10 20.7, R@50 55.0 MedR: **42**, MeanR: **91.9**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lalignment, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.03528064442431833\n\nTest : Video to Text, R@1: 5.5, R@5: 17.9, R@10 28.4, R@50 64.4 MedR: 30, MeanR: **71.2**\n\nTest : Text to Video, R@1: 3.7, R@5: 11.9, R@10 20.7, R@50 55.3 MedR: 43, MeanR: **91.7**","metadata":{}},{"cell_type":"markdown","source":"**Model with Lsupervised + Lpseudo + Lalignment, OpenAI random labels, margin=0.1**\n\nCheckpoint from: 50 epoch  with best loss 0.033720782422643945\n\nTest : Video to Text, R@1: **5.6**, R@5: 19.5, R@10 28.2, R@50 63.7 MedR: 31, MeanR: **70.8**\n\nTest : Text to Video, R@1: **4.1**, R@5: 13.5, R@10 20.7, R@50 58.3 MedR: 36, MeanR: **84.7**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}